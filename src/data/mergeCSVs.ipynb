{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "path_base = os.path.dirname(os.path.dirname(os.getcwd()))  # Base path of the repository\n",
    "sys.path.append(path_base)  # Add base path to system path\n",
    "\n",
    "from utils.logging import logger_setup, log_section\n",
    "from utils.merge import log_dataframe_info, log_merge_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None) # Show all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_data = os.path.join(path_base, \"data\", \"raw\")\n",
    "path_interim_data = os.path.join(path_base, \"data\", \"interim\")\n",
    "\n",
    "os.makedirs(path_interim_data, exist_ok=True) # Ensure the interim directory exists\n",
    "\n",
    "path_logs = os.path.join(path_interim_data, \"logs\") # Set up log directory\n",
    "os.makedirs(path_logs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our main logger\n",
    "main_logger = logger_setup(\"InfoMerge\", path_logs, \"infoMerge.log\")\n",
    "main_logger.info(\"Starting data merge process\")\n",
    "main_logger.info(f\"Reading data from: {path_raw_data}\")\n",
    "main_logger.info(f\"Saving results to: {path_interim_data}\")\n",
    "\n",
    "def check_missing_threshold(df, key_columns, df_name, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Check if missing values in key columns exceed the threshold\n",
    "        - If exceed threshold, will return False and we should stop processing\n",
    "        - If not exceeded threshold, will return True and we can proceed\n",
    "    \n",
    "    Params:\n",
    "        - df (DataFrame): DataFrame to check for missing values\n",
    "        - key_columns (list): List of columns to check for missing values\n",
    "        - df_name (str): Name of the DataFrame for logging purposes\n",
    "        - threshold (float): Threshold for missing value percentage\n",
    "    \n",
    "    Returns:\n",
    "        - bool: True if missing percentage is below threshold, False otherwise\n",
    "        - float: Percentage of missing values\n",
    "        - DataFrame: Rows with missing values in key columns\n",
    "    \"\"\"\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    rows_with_missing = df[df[key_columns].isnull().any(axis=1)]\n",
    "    missing_count = len(rows_with_missing)\n",
    "    missing_percentage = missing_count / total_rows if total_rows > 0 else 0\n",
    "    \n",
    "    main_logger.info(f\"\")\n",
    "    main_logger.info(f\"Checking missing values in {df_name} key columns: {key_columns}\")\n",
    "    main_logger.info(f\"  Total rows in {df_name}: {total_rows}\")\n",
    "    main_logger.info(f\"  Rows with missing values in key columns: {missing_count}\")\n",
    "    main_logger.info(f\"  Missing percentage: {missing_percentage:.4f} (threshold: {threshold:.4f})\")\n",
    "    \n",
    "    return missing_percentage < threshold, missing_percentage, rows_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d056585",
   "metadata": {},
   "source": [
    "### LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data files\n",
    "path_data = os.path.join(path_raw_data, \"data.csv\")\n",
    "path_translator_pairs = os.path.join(path_raw_data, \"translatorsCostPairs.csv\")\n",
    "path_schedules = os.path.join(path_raw_data, \"schedules.csv\")\n",
    "path_clients = os.path.join(path_raw_data, \"clients.csv\")\n",
    "\n",
    "df_data = pd.read_csv(path_data)  # Historical data containing project information\n",
    "df_translator_pairs = pd.read_csv(path_translator_pairs)  # Current translator rates\n",
    "df_schedules = pd.read_csv(path_schedules)  # Translator schedules\n",
    "df_clients = pd.read_csv(path_clients)  # Client information\n",
    "\n",
    "main_logger.info(f\"Loaded {len(df_data)} rows from data.csv\")\n",
    "main_logger.info(f\"Loaded {len(df_translator_pairs)} rows from translatorsCostPairs.csv\")\n",
    "main_logger.info(f\"Loaded {len(df_schedules)} rows from schedules.csv\")\n",
    "main_logger.info(f\"Loaded {len(df_clients)} rows from clients.csv\")\n",
    "\n",
    "# Data overview\n",
    "main_logger.info(\"Data Overview:\")\n",
    "main_logger.info(f\"- data.csv: {df_data.shape[0]} rows, {df_data.shape[1]} columns\")\n",
    "main_logger.info(f\"- translatorsCostPairs.csv: {df_translator_pairs.shape[0]} rows, {df_translator_pairs.shape[1]} columns\")\n",
    "main_logger.info(f\"- schedules.csv: {df_schedules.shape[0]} rows, {df_schedules.shape[1]} columns\")\n",
    "main_logger.info(f\"- clients.csv: {df_clients.shape[0]} rows, {df_clients.shape[1]} columns\")\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = df_data.isnull().sum() # Analyze missing values in data.csv\n",
    "main_logger.info(f\"\\nMissing values in data.csv: \\n\\n{missing_data}\\n\")\n",
    "\n",
    "missing_translator_pairs = df_translator_pairs.isnull().sum() # Analyze missing values in translatorsCostPairs.csv\n",
    "main_logger.info(f\"\\nMissing values in translatorsCostPairs.csv: \\n\\n{missing_translator_pairs}\\n\")\n",
    "\n",
    "missing_schedules = df_schedules.isnull().sum() # Analyze missing values in schedules.csv\n",
    "main_logger.info(f\"\\nMissing values in schedules.csv: \\n\\n{missing_schedules}\\n\")\n",
    "\n",
    "missing_clients = df_clients.isnull().sum() # Analyze missing values in clients.csv\n",
    "main_logger.info(f\"\\nMissing values in clients.csv: \\n\\n{missing_clients}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2839a6c",
   "metadata": {},
   "source": [
    "### ANALYSIS QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and clean missing values in key columns for data.csv\n",
    "data_key_columns = [\"TRANSLATOR\", \"SOURCE_LANG\", \"TARGET_LANG\", \"MANUFACTURER\"]\n",
    "can_proceed_data, missing_pct_data, rows_deletion = check_missing_threshold(\n",
    "    df_data, data_key_columns, \"data.csv\"\n",
    ")\n",
    "\n",
    "# Log details of rows with missing key values\n",
    "if len(rows_deletion) > 0:\n",
    "    main_logger.info(\"  Rows with missing values in key columns for data.csv:\")\n",
    "    for idx, row in rows_deletion.iterrows():\n",
    "        missing_cols = [col for col in data_key_columns if pd.isnull(row[col])]\n",
    "        missing_cols_str = \", \".join(missing_cols)\n",
    "        main_logger.info(f\"    - Row {idx}: Missing values in {missing_cols_str}\")\n",
    "\n",
    "# Check translatorsCostPairs.csv\n",
    "translator_key_columns = [\"TRANSLATOR\", \"SOURCE_LANG\", \"TARGET_LANG\"]\n",
    "can_proceed_translator, missing_pct_translator, rows_to_remove_translator = check_missing_threshold(\n",
    "    df_translator_pairs, translator_key_columns, \"translatorsCostPairs.csv\"\n",
    ")\n",
    "\n",
    "# Check schedules.csv\n",
    "schedules_key_columns = [\"NAME\"]\n",
    "can_proceed_schedules, missing_pct_schedules, rows_to_remove_schedules = check_missing_threshold(\n",
    "    df_schedules, schedules_key_columns, \"schedules.csv\"\n",
    ")\n",
    "\n",
    "# Check clients.csv\n",
    "clients_key_columns = [\"CLIENT_NAME\"]\n",
    "can_proceed_clients, missing_pct_clients, rows_to_remove_clients = check_missing_threshold(\n",
    "    df_clients, clients_key_columns, \"clients.csv\"\n",
    ")\n",
    "\n",
    "# Determine if we can proceed with the merge\n",
    "can_proceed_overall = can_proceed_data and can_proceed_translator and can_proceed_schedules and can_proceed_clients\n",
    "\n",
    "if not can_proceed_overall:\n",
    "    main_logger.error(\"DATA QUALITY ERROR: Missing values in key columns exceed the 5% threshold\")\n",
    "    \n",
    "    if not can_proceed_data:\n",
    "        main_logger.error(f\"data.csv: {missing_pct_data:.2%} rows missing values in {data_key_columns}\")\n",
    "    \n",
    "    if not can_proceed_translator:\n",
    "        main_logger.error(f\"translatorsCostPairs.csv: {missing_pct_translator:.2%} rows missing values in {translator_key_columns}\")\n",
    "    \n",
    "    if not can_proceed_schedules:\n",
    "        main_logger.error(f\"schedules.csv: {missing_pct_schedules:.2%} rows missing values in {schedules_key_columns}\")\n",
    "    \n",
    "    if not can_proceed_clients:\n",
    "        main_logger.error(f\"clients.csv: {missing_pct_clients:.2%} rows missing values in {clients_key_columns}\")\n",
    "    \n",
    "    main_logger.error(\"TERMINATING MERGE PROCESS... DUE TO DATA QUALITY ISSUES CHECK THEM BEFORE MERGING\")\n",
    "    raise SystemExit(\"Exiting due to data quality errors in key columns\")\n",
    "else:\n",
    "    main_logger.info(\"\")\n",
    "    main_logger.info(\"PROCEEDING WITH DATA CLEANING... (ALL KEY COLUMNS HAVE ACCEPTABLE MISSING VALUE RATES)\")\n",
    "    main_logger.info(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb9bfc",
   "metadata": {},
   "source": [
    "### CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d96692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows that need to be removed from data.csv\n",
    "rows_deletion = rows_deletion.drop_duplicates()\n",
    "rows_before_total = len(df_data)\n",
    "\n",
    "df_data.dropna(inplace=True)\n",
    "rows_after_total = len(df_data)\n",
    "\n",
    "deletion_total = rows_before_total - rows_after_total\n",
    "\n",
    "main_logger.info(f\"Removed {deletion_total} rows with missing values from data.csv\")\n",
    "main_logger.info(f\"Remaining rows after cleaning: {rows_after_total} ({(rows_after_total/rows_before_total)*100:.5f}% of original)\")\n",
    "\n",
    "# Show details of removed rows\n",
    "if deletion_total > 0:\n",
    "    main_logger.info(f\"\\nDetails of removed rows: \\n\\n{rows_deletion[['PROJECT_ID', 'TRANSLATOR', 'SOURCE_LANG', 'TARGET_LANG', 'MANUFACTURER']].to_string()}\\n\")\n",
    "\n",
    "# Clean the translator pairs dataframe if needed\n",
    "if len(rows_to_remove_translator) > 0:\n",
    "    translator_rows_before = len(df_translator_pairs)\n",
    "    df_translator_pairs = df_translator_pairs.dropna(subset=translator_key_columns)\n",
    "    translator_rows_after = len(df_translator_pairs)\n",
    "    translator_removed = translator_rows_before - translator_rows_after\n",
    "    \n",
    "    main_logger.info(f\"Removed {translator_removed} rows with missing values from translatorsCostPairs.csv\")\n",
    "\n",
    "# Clean the schedules dataframe if needed\n",
    "if len(rows_to_remove_schedules) > 0:\n",
    "    schedules_rows_before = len(df_schedules)\n",
    "    df_schedules = df_schedules.dropna(subset=schedules_key_columns)\n",
    "    schedules_rows_after = len(df_schedules)\n",
    "    schedules_removed = schedules_rows_before - schedules_rows_after\n",
    "    \n",
    "    main_logger.info(f\"Removed {schedules_removed} rows with missing values from schedules.csv\")\n",
    "\n",
    "# Clean the clients dataframe if needed\n",
    "if len(rows_to_remove_clients) > 0:\n",
    "    clients_rows_before = len(df_clients)\n",
    "    df_clients = df_clients.dropna(subset=clients_key_columns)\n",
    "    clients_rows_after = len(df_clients)\n",
    "    clients_removed = clients_rows_before - clients_rows_after\n",
    "    \n",
    "    main_logger.info(f\"Removed {clients_removed} rows with missing values from clients.csv\")\n",
    "\n",
    "main_logger.info(f\"\\nSample of data.csv (after cleaning): \\n\\n{df_data.head().to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e06d02",
   "metadata": {},
   "source": [
    "### MERGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954753e",
   "metadata": {},
   "source": [
    "##### Translator Costs Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the translator merge\n",
    "translator_logger = logger_setup(\"TranslatorMerge\", path_logs, \"translatorsCostPairsMerge.log\")\n",
    "log_section(translator_logger, \"PROCESSING TRANSLATOR COST PAIRS MERGE\")\n",
    "\n",
    "log_dataframe_info(translator_logger, df_data, \"Cleaned data\")\n",
    "log_dataframe_info(translator_logger, df_translator_pairs, \"Translator pairs\")\n",
    "\n",
    "# Composite key for identifying unique translator-language pairs\n",
    "df_data[\"PAIR_KEY\"] = df_data.apply(\n",
    "    lambda row: (row[\"TRANSLATOR\"], row[\"SOURCE_LANG\"], row[\"TARGET_LANG\"]), \n",
    "    axis=1\n",
    ")\n",
    "df_translator_pairs[\"PAIR_KEY\"] = df_translator_pairs.apply(\n",
    "    lambda row: (row[\"TRANSLATOR\"], row[\"SOURCE_LANG\"], row[\"TARGET_LANG\"]), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "set_existing_pairs = set(df_data[\"PAIR_KEY\"]) # Find existing pairs in historical data\n",
    "set_translator_pairs = set(df_translator_pairs[\"PAIR_KEY\"]) # Find pairs in translator cost pairs\n",
    "set_missing_pairs = set_translator_pairs - set_existing_pairs\n",
    "\n",
    "# Log missing pairs information\n",
    "translator_logger.info(f\"Total pairs in data.csv: {len(set_existing_pairs)}\")\n",
    "translator_logger.info(f\"Total pairs in translatorsCostPairs.csv: {len(set_translator_pairs)}\")\n",
    "translator_logger.info(f\"Total missing pairs: {len(set_missing_pairs)}\")\n",
    "\n",
    "\n",
    "if len(set_missing_pairs) > 0:\n",
    "    translator_logger.info(\"Missing Pairs (sample):\")\n",
    "    missing_pairs_list = []\n",
    "    for pair in list(set_missing_pairs)[:10]: # Log first top missing pairs\n",
    "        translator, source, target = pair  # Direct tuple unpacking without splitting\n",
    "        missing_pairs_list.append({\n",
    "            \"TRANSLATOR\": translator,\n",
    "            \"SOURCE_LANG\": source,\n",
    "            \"TARGET_LANG\": target\n",
    "        })\n",
    "        translator_logger.info(f\"  TRANSLATOR: {translator}, SOURCE_LANG: {source}, TARGET_LANG: {target}\")\n",
    "    \n",
    "    df_missing_pairs = pd.DataFrame(missing_pairs_list)\n",
    "    translator_logger.info(f\"\\nSample of Missing Pairs: {df_missing_pairs.to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286cc67",
   "metadata": {},
   "source": [
    "##### Translators Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary structures for easy querying\n",
    "dict_translator_langs = {} # Dictionary by translator\n",
    "dict_source_langs = {} # Dictionary by source languages\n",
    "dict_target_langs = {} # Dictionary by target languages\n",
    "\n",
    "translator_logger.info(\"Building language dictionaries...\")\n",
    "\n",
    "# Populate the dictionaries\n",
    "for _, row in df_translator_pairs.iterrows():\n",
    "    str_translator = row[\"TRANSLATOR\"]\n",
    "    str_source = row[\"SOURCE_LANG\"]\n",
    "    str_target = row[\"TARGET_LANG\"]\n",
    "    num_rate = row[\"HOURLY_RATE\"]\n",
    "    \n",
    "    if str_translator not in dict_translator_langs: # Add to translator dictionary\n",
    "        dict_translator_langs[str_translator] = {}\n",
    "    if str_source not in dict_translator_langs[str_translator]: \n",
    "        dict_translator_langs[str_translator][str_source] = {}\n",
    "    dict_translator_langs[str_translator][str_source][str_target] = num_rate\n",
    "    \n",
    "    if str_source not in dict_source_langs: # Add to source language dictionary\n",
    "        dict_source_langs[str_source] = {}\n",
    "    if str_target not in dict_source_langs[str_source]: \n",
    "        dict_source_langs[str_source][str_target] = {}\n",
    "    dict_source_langs[str_source][str_target][str_translator] = num_rate\n",
    "    \n",
    "    if str_target not in dict_target_langs: # Add to target language dictionary\n",
    "        dict_target_langs[str_target] = {}\n",
    "    if str_source not in dict_target_langs[str_target]:\n",
    "        dict_target_langs[str_target][str_source] = {}\n",
    "    dict_target_langs[str_target][str_source][str_translator] = num_rate\n",
    "\n",
    "translator_logger.info(\"Dictionary Structure Summary:\")\n",
    "translator_logger.info(f\"  - Translators: {len(dict_translator_langs)}\")\n",
    "translator_logger.info(f\"  - Source languages: {len(dict_source_langs)}\")\n",
    "translator_logger.info(f\"  - Target languages: {len(dict_target_langs)}\")\n",
    "\n",
    "df_merged = df_data.copy() # Create a new dataframe with optimized structure\n",
    "\n",
    "\n",
    "translator_logger.info(\"Adding structured language pairs data...\") # Add the structured language pairs column\n",
    "\n",
    "\n",
    "# TODO - Do not consider this columns since the dictionaries are faster and the only purpose is to check the newest information of a translator\n",
    "#\n",
    "# def extract_translator_langs(str_translator_name):\n",
    "#     \"\"\"\n",
    "#     Extracts language pairs for a given translator from the dictionary structure\n",
    "#    \n",
    "#     Params:\n",
    "#         - str_translator_name (str): Name of the translator to extract pairs for\n",
    "#   \n",
    "#     Returns:\n",
    "#         - list: List of dictionaries containing source, target languages and hourly rates\n",
    "#     \"\"\"\n",
    "#     if str_translator_name not in dict_translator_langs:\n",
    "#         return []  # Return empty list if translator not found\n",
    "#    \n",
    "#     list_pairs = []\n",
    "#     dict_sources = dict_translator_langs[str_translator_name]\n",
    "#    \n",
    "#     for str_source, dict_targets in dict_sources.items():\n",
    "#         for str_target, num_rate in dict_targets.items():\n",
    "#             list_pairs.append({\n",
    "#                 \"source_lang\": str_source,\n",
    "#                 \"target_lang\": str_target,\n",
    "#                 \"hourly_rate\": num_rate,\n",
    "#                 \"in_historical_data\": f\"{str_translator_name}||{str_source}||{str_target}\" in set_existing_pairs\n",
    "#             })\n",
    "#    \n",
    "#     return list_pairs\n",
    "# df_merged[\"TRANSLATOR_LANGUAGE_PAIRS\"] = df_merged[\"TRANSLATOR\"].apply(extract_translator_langs)\n",
    "# df_merged[\"TRANSLATOR_LANGUAGE_PAIRS_JSON\"] = df_merged[\"TRANSLATOR_LANGUAGE_PAIRS\"].apply(json.dumps) # Convert to JSON for easier storage/viewing\n",
    "\n",
    "\n",
    "def get_specific_pair_rate(row): # Function to get specific pair rates\n",
    "    str_translator = row[\"TRANSLATOR\"]\n",
    "    str_source = row[\"SOURCE_LANG\"]\n",
    "    str_target = row[\"TARGET_LANG\"]\n",
    "    \n",
    "    # Check if the translator exists in the dictionary    \n",
    "    if (str_translator in dict_translator_langs and \n",
    "        str_source in dict_translator_langs[str_translator] and\n",
    "        str_target in dict_translator_langs[str_translator][str_source]):\n",
    "        return dict_translator_langs[str_translator][str_source][str_target]\n",
    "    \n",
    "    return None\n",
    "\n",
    "translator_logger.info(\"Adding latest hourly rates...\")\n",
    "df_merged[\"TRANSLATOR_HOURLY_RATE_LATEST\"] = df_merged.apply(get_specific_pair_rate, axis=1)\n",
    "df_merged[\"DISCREPANCY_HOURLY_RATE\"] = df_merged[\"HOURLY_RATE\"] - df_merged[\"TRANSLATOR_HOURLY_RATE_LATEST\"] # Compare with the original hourly rate\n",
    "\n",
    "# Log rate discrepancies\n",
    "discrepancies = df_merged[df_merged[\"DISCREPANCY_HOURLY_RATE\"].notnull() & \n",
    "                          (df_merged[\"DISCREPANCY_HOURLY_RATE\"] != 0)]\n",
    "\n",
    "translator_logger.info(f\"Rate Discrepancies Found: {len(discrepancies)}\")\n",
    "\n",
    "# Additional analysis for notebook\n",
    "main_logger.info(f\"Rate Discrepancies Found: {len(discrepancies)}\")\n",
    "if len(discrepancies) > 0:\n",
    "    main_logger.info(f\"Sample discrepancies: \\n\\n{discrepancies[['TRANSLATOR', 'SOURCE_LANG', 'TARGET_LANG', 'HOURLY_RATE', 'TRANSLATOR_HOURLY_RATE_LATEST', 'DISCREPANCY_HOURLY_RATE']].head().to_string()}\\n\")\n",
    "\n",
    "log_section(translator_logger, \"TRANSLATOR COST PAIRS MERGE COMPLETE\")\n",
    "main_logger.info(\"Translator cost pairs merge completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45551ed",
   "metadata": {},
   "source": [
    "##### Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_mappings = {\n",
    "    \"MON\": \"MONDAY\",\n",
    "    \"TUES\": \"TUESDAY\",\n",
    "    \"WED\": \"WEDNESDAY\",\n",
    "    \"THURS\": \"THURSDAY\",\n",
    "    \"FRI\": \"FRIDAY\",\n",
    "    \"SAT\": \"SATURDAY\",\n",
    "    \"SUN\": \"SUNDAY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the schedules merge\n",
    "schedules_logger = logger_setup(\"SchedulesMerge\", path_logs, \"schedulesMerge.log\")\n",
    "log_section(schedules_logger, \"PROCESSING SCHEDULES MERGE\")\n",
    "\n",
    "log_dataframe_info(schedules_logger, df_merged, \"Merged data (with translator pairs)\")\n",
    "log_dataframe_info(schedules_logger, df_schedules, \"Schedules data\")\n",
    "\n",
    "schedules_logger.info(\"Renaming columns for schedules merge...\")\n",
    "df_schedules_renamed = df_schedules.copy()\n",
    "\n",
    "# Add prefix to schedule columns to rename them\n",
    "for col in df_schedules_renamed.columns:\n",
    "    if col != \"NAME\": # Keep NAME for merging\n",
    "        if col in day_mappings:\n",
    "            new_name = f\"SCHEDULE_{day_mappings[col]}\"\n",
    "            print(f\"Renaming column {col} to {new_name}\")\n",
    "            df_schedules_renamed.rename(columns={col: new_name}, inplace=True)\n",
    "        else:\n",
    "            print(f\"Renaming column {col} to SCHEDULE_{col}\")\n",
    "            df_schedules_renamed.rename(columns={col: f\"SCHEDULE_{col}\"}, inplace=True)\n",
    "\n",
    "df_schedules_renamed.rename(columns={ # Rename availability dates\n",
    "    \"SCHEDULE_START\": \"SCHEDULE_START_AVAILABLE\",\n",
    "    \"SCHEDULE_END\": \"SCHEDULE_END_AVAILABLE\"\n",
    "}, inplace=True)\n",
    "\n",
    "df_merged.rename(columns={ # Rename task dates in merged data\n",
    "    \"START\": \"START_TASK\",\n",
    "    \"END\": \"END_TASK\",\n",
    "    \"DELIVERED\": \"DELIVERED_TASK\",\n",
    "    \"READY\": \"READY_TASK\",\n",
    "    \"WORKING\": \"WORKING_TASK\",\n",
    "    \"RECEIVED\": \"RECEIVED_TASK\",\n",
    "    \"CLOSE\": \"CLOSE_TASK\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Log column renaming results\n",
    "schedules_logger.info(\"Column Renaming Results:\")\n",
    "schedules_logger.info(f\"  - Original schedule columns: {list(df_schedules.columns)}\")\n",
    "schedules_logger.info(f\"  - Renamed schedule columns: {list(df_schedules_renamed.columns)}\")\n",
    "schedules_logger.info(f\"  - Data date columns renamed: START -> START_TASK, END -> END_TASK\")\n",
    "\n",
    "# Check for missing translators\n",
    "set_data_translators = set(df_merged[\"TRANSLATOR\"])\n",
    "set_schedule_translators = set(df_schedules_renamed[\"NAME\"])\n",
    "set_missing_schedules = set_data_translators - set_schedule_translators\n",
    "\n",
    "schedules_logger.info(f\"Translators in data.csv: {len(set_data_translators)}\")\n",
    "schedules_logger.info(f\"Translators in schedules.csv: {len(set_schedule_translators)}\")\n",
    "schedules_logger.info(f\"Translators without schedules: {len(set_missing_schedules)}\")\n",
    "\n",
    "if len(set_missing_schedules) > 0:\n",
    "    schedules_logger.info(\"Sample of translators without schedule information:\")\n",
    "    df_missing_schedules = pd.DataFrame(list(set_missing_schedules), columns=[\"TRANSLATOR\"])\n",
    "    main_logger.info(f\"\\nSample of translators without schedule information: {df_missing_schedules.to_string()}\")\n",
    "\n",
    "# Merge Dataframes \n",
    "schedules_logger.info(\"Performing merge operation...\")\n",
    "df_merged_with_schedules = pd.merge(\n",
    "    df_merged,\n",
    "    df_schedules_renamed,\n",
    "    left_on=\"TRANSLATOR\", # Left join to keep all rows from data.csv\n",
    "    right_on=\"NAME\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "log_merge_stats( # Log merge statistics\n",
    "    schedules_logger, \n",
    "    df_merged, \n",
    "    df_schedules_renamed,\n",
    "    df_merged_with_schedules,\n",
    "    \"TRANSLATOR\",\n",
    "    \"NAME\"\n",
    ")\n",
    "\n",
    "# Calculate merge statistics\n",
    "num_total_rows = len(df_merged_with_schedules)\n",
    "num_with_schedules = df_merged_with_schedules[\"NAME\"].notnull().sum()\n",
    "num_without_schedules = num_total_rows - num_with_schedules\n",
    "\n",
    "schedules_logger.info(f\"Detailed Merge Results:\")\n",
    "schedules_logger.info(f\"  - Total rows after merge: {num_total_rows}\")\n",
    "schedules_logger.info(f\"  - Rows with schedule data: {num_with_schedules}\")\n",
    "schedules_logger.info(f\"  - Rows without schedule data: {num_without_schedules}\")\n",
    "schedules_logger.info(f\"  - Percentage with schedule data: {(num_with_schedules/num_total_rows)*100:.2f}%\")\n",
    "\n",
    "\n",
    "df_merged_with_schedules.drop(columns=[\"NAME\"], inplace=True, errors=\"ignore\") # Drop duplicate NAME column\n",
    "\n",
    "schedules_logger.info(\"Dropped duplicate NAME column from merged result\")\n",
    "main_logger.info(\"Dropped duplicate NAME column from merged result\")\n",
    "\n",
    "log_section(schedules_logger, \"SCHEDULES MERGE COMPLETE\")\n",
    "main_logger.info(\"Schedules merge completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c230f2",
   "metadata": {},
   "source": [
    "##### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a67807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the clients merge\n",
    "clients_logger = logger_setup(\"ClientsMerge\", path_logs, \"clientsMerge.log\")\n",
    "log_section(clients_logger, \"PROCESSING CLIENTS MERGE\")\n",
    "\n",
    "log_dataframe_info(clients_logger, df_merged_with_schedules, \"Merged data (with schedules)\")\n",
    "log_dataframe_info(clients_logger, df_clients, \"Clients data\")\n",
    "\n",
    "clients_logger.info(\"Renaming columns for clients merge...\")\n",
    "df_clients_renamed = df_clients.copy()\n",
    "df_clients_renamed.rename(columns={ # Rename specific client columns\n",
    "    \"SELLING_HOURLY_PRICE\": \"CLIENT_HOURLY_PRICE\",\n",
    "    \"MIN_QUALITY\": \"CLIENT_MIN_QUALITY\",\n",
    "    \"WILDCARD\": \"CLIENT_WILDCARD\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Add prefix to remaining client columns\n",
    "for col in df_clients_renamed.columns:\n",
    "    if col == \"CLIENT_NAME\" or col.startswith(\"CLIENT_\"):  # Keep CLIENT_NAME for merging\n",
    "        continue\n",
    "    df_clients_renamed.rename(columns={col: f\"CLIENT_{col}\"}, inplace=True)\n",
    "\n",
    "# Log column renaming results\n",
    "clients_logger.info(\"Column Renaming Results:\")\n",
    "clients_logger.info(f\"  - Original client columns: {list(df_clients.columns)}\")\n",
    "clients_logger.info(f\"  - Renamed client columns: {list(df_clients_renamed.columns)}\")\n",
    "\n",
    "# Check for missing manufacturers\n",
    "set_manufacturers = set(df_merged_with_schedules[\"MANUFACTURER\"])\n",
    "set_clients = set(df_clients_renamed[\"CLIENT_NAME\"])\n",
    "set_missing_clients = set_manufacturers - set_clients\n",
    "\n",
    "clients_logger.info(f\"Manufacturers in data.csv: {len(set_manufacturers)}\")\n",
    "clients_logger.info(f\"Clients in clients.csv: {len(set_clients)}\")\n",
    "clients_logger.info(f\"Manufacturers without client information: {len(set_missing_clients)}\")\n",
    "\n",
    "if len(set_missing_clients) > 0:\n",
    "    df_missing_clients = pd.DataFrame(list(set_missing_clients)[:10], columns=[\"Manufacturer\"])\n",
    "    clients_logger.info(f\"\\nSample of manufacturers without client information: \\n\\n{df_missing_clients.to_string()}\\n\")\n",
    "\n",
    "# Merge Dataframes \n",
    "clients_logger.info(\"Performing merge operation...\")\n",
    "df_final = pd.merge(\n",
    "    df_merged_with_schedules,\n",
    "    df_clients_renamed,\n",
    "    left_on=\"MANUFACTURER\", # Left join to keep all rows from merged data\n",
    "    right_on=\"CLIENT_NAME\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "log_merge_stats( # Log merge statistics\n",
    "    clients_logger, \n",
    "    df_merged_with_schedules, \n",
    "    df_clients_renamed,\n",
    "    df_final,\n",
    "    \"MANUFACTURER\",\n",
    "    \"CLIENT_NAME\"\n",
    ")\n",
    "\n",
    "# Calculate merge statistics\n",
    "num_total_rows = len(df_final)\n",
    "num_with_clients = df_final[\"CLIENT_NAME\"].notnull().sum()\n",
    "num_without_clients = num_total_rows - num_with_clients\n",
    "\n",
    "clients_logger.info(f\"Detailed Merge Results:\")\n",
    "clients_logger.info(f\"  - Total rows after merge: {num_total_rows}\")\n",
    "clients_logger.info(f\"  - Rows with client data: {num_with_clients}\")\n",
    "clients_logger.info(f\"  - Rows without client data: {num_without_clients}\")\n",
    "clients_logger.info(f\"  - Percentage with client data: {(num_with_clients/num_total_rows)*100:.2f}%\")\n",
    "\n",
    "# Add quality comparison\n",
    "clients_logger.info(\"Checking client quality requirements...\")\n",
    "df_final[\"MEETS_CLIENT_QUALITY\"] = pd.NA  # Use pandas NA which respects dtypes\n",
    "mask = df_final[\"CLIENT_MIN_QUALITY\"].notnull() & df_final[\"QUALITY_EVALUATION\"].notnull()\n",
    "# Create the comparison result as a series first, then assign it\n",
    "comparison_result = df_final.loc[mask, \"QUALITY_EVALUATION\"] >= df_final.loc[mask, \"CLIENT_MIN_QUALITY\"]\n",
    "df_final.loc[mask, \"MEETS_CLIENT_QUALITY\"] = comparison_result\n",
    "\n",
    "# Count quality issues\n",
    "quality_issues = df_final[(df_final[\"MEETS_CLIENT_QUALITY\"] == False)].shape[0]\n",
    "clients_logger.info(f\"Quality Assessment:\")\n",
    "clients_logger.info(f\"  - Tasks not meeting client minimum quality: {quality_issues}\")\n",
    "\n",
    "if quality_issues > 0:\n",
    "    clients_logger.info(f\"  - Percentage of quality issues: {(quality_issues/num_with_clients)*100:.2f}%\")\n",
    "    \n",
    "    quality_data = df_final[mask].copy()\n",
    "    quality_data[\"Quality_Status\"] = quality_data[\"MEETS_CLIENT_QUALITY\"].map({True: \"Meets\", False: \"Below\"})\n",
    "    \n",
    "    # Show quality by client\n",
    "    quality_summary = quality_data.groupby(\"CLIENT_NAME\").agg(\n",
    "        Total=(\"CLIENT_NAME\", \"count\"),\n",
    "        MeetsQuality=(\"MEETS_CLIENT_QUALITY\", lambda x: sum(x)),\n",
    "        BelowQuality=(\"MEETS_CLIENT_QUALITY\", lambda x: sum(~x))\n",
    "    ).sort_values(\"Total\", ascending=False)\n",
    "    \n",
    "    # Display quality summary\n",
    "    clients_logger.info(f\"\\nQuality issues by client: \\n\\n{quality_summary.head().to_string()}\\n\")\n",
    "\n",
    "# After all logging and analysis is complete\n",
    "clients_logger.info(\"Removing redundant CLIENT_NAME column (same as MANUFACTURER)...\")\n",
    "df_final = df_final.drop(columns=[\"CLIENT_NAME\"])\n",
    "\n",
    "log_section(clients_logger, \"CLIENTS MERGE COMPLETE\")\n",
    "main_logger.info(\"Clients merge completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692ca23",
   "metadata": {},
   "source": [
    "##### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_order = [\n",
    "    \"PROJECT_ID\", \"TASK_ID\", # Project Identification Columns\n",
    "    \"PM\", \"TASK_TYPE\", # Management Information\n",
    "    \"START_TASK\", \"END_TASK\", \"ASSIGNED\", \"READY_TASK\", \"WORKING_TASK\", # Timeline Information\n",
    "    \"DELIVERED_TASK\", \"RECEIVED_TASK\", \"CLOSE_TASK\",\n",
    "    \"SOURCE_LANG\", \"TARGET_LANG\", \"PAIR_KEY\", # Language Pair Information\n",
    "    \"TRANSLATOR\", \"QUALITY_EVALUATION\", # Translator Information\n",
    "    \"SCHEDULE_START_AVAILABLE\", \"SCHEDULE_END_AVAILABLE\", # Translator Schedule Information\n",
    "    \"SCHEDULE_MONDAY\", \"SCHEDULE_TUESDAY\", \"SCHEDULE_WEDNESDAY\", \n",
    "    \"SCHEDULE_THURSDAY\", \"SCHEDULE_FRIDAY\", \"SCHEDULE_SATURDAY\", \"SCHEDULE_SUNDAY\",\n",
    "    \"FORECAST\", \"HOURLY_RATE\", \"COST\", \"TRANSLATOR_HOURLY_RATE_LATEST\", # Financial Information\n",
    "    \"DISCREPANCY_HOURLY_RATE\", \"CLIENT_HOURLY_PRICE\",\n",
    "    \"MANUFACTURER\", \"MANUFACTURER_SECTOR\", \"MANUFACTURER_INDUSTRY_GROUP\", # Client Information\n",
    "    \"MANUFACTURER_INDUSTRY\", \"MANUFACTURER_SUBINDUSTRY\", \n",
    "    \"CLIENT_MIN_QUALITY\", \"CLIENT_WILDCARD\",\n",
    "    \"MEETS_CLIENT_QUALITY\" # Analytical Columns\n",
    "]\n",
    "\n",
    "df_final = df_final[new_column_order]\n",
    "\n",
    "# Inspect the first few rows with new column order\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcffb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum null values\n",
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_logger.info(\"Saving merge results...\")\n",
    "\n",
    "path_merged_csv = os.path.join(path_interim_data, \"mergedAll.csv\")\n",
    "df_final.to_csv(path_merged_csv, index=False) # Save the final merged CSV\n",
    "main_logger.info(f\"Saved merged data to {path_merged_csv}\")\n",
    "\n",
    "\n",
    "# Save the dictionary structures to JSON for reference\n",
    "path_translator_langs_json = os.path.join(path_interim_data, \"translatorLanguages.json\")\n",
    "with open(path_translator_langs_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict_translator_langs, f, indent=4, ensure_ascii=False)\n",
    "main_logger.info(f\"Saved translator languages dictionary to {path_translator_langs_json}\")\n",
    "\n",
    "path_source_langs_json = os.path.join(path_interim_data, \"sourceLanguages.json\")\n",
    "with open(path_source_langs_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict_source_langs, f, indent=4, ensure_ascii=False)\n",
    "main_logger.info(f\"Saved source languages dictionary to {path_source_langs_json}\")\n",
    "\n",
    "path_target_langs_json = os.path.join(path_interim_data, \"targetLanguages.json\")\n",
    "with open(path_target_langs_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dict_target_langs, f, indent=4, ensure_ascii=False)\n",
    "main_logger.info(f\"Saved target languages dictionary to {path_target_langs_json}\")\n",
    "\n",
    "# Summary statistics\n",
    "log_section(main_logger, \"PROCESSING COMPLETE\")\n",
    "main_logger.info(f\"Original data rows before cleaning: {rows_before_total}\")\n",
    "main_logger.info(f\"Rows removed due to missing values: {deletion_total}\")\n",
    "main_logger.info(f\"Data rows after cleaning: {rows_after_total}\")\n",
    "main_logger.info(f\"Final merged rows: {len(df_final)}\")\n",
    "main_logger.info(f\"Final columns count: {len(df_final.columns)}\")\n",
    "main_logger.info(f\"Translators: {len(set_data_translators)}\")\n",
    "main_logger.info(\"Languages covered:\")\n",
    "main_logger.info(f\"  - Source languages: {len(dict_source_langs)}\")\n",
    "main_logger.info(f\"  - Target languages: {len(dict_target_langs)}\")\n",
    "main_logger.info(f\"Manufacturers/clients: {len(set_manufacturers)}\")\n",
    "main_logger.info(\"Check the logs directory for detailed merge information.\")\n",
    "\n",
    "# Additional notebook analysis\n",
    "main_logger.info(\"Data Merge Summary:\")\n",
    "main_logger.info(f\"  - Original data rows before cleaning: {rows_before_total}\")\n",
    "main_logger.info(f\"  - Rows removed due to missing values: {deletion_total}\")\n",
    "main_logger.info(f\"  - Data rows after cleaning: {rows_after_total}\")\n",
    "main_logger.info(f\"  - Final merged rows: {len(df_final)}\")\n",
    "main_logger.info(f\"  - Final columns count: {len(df_final.columns)}\")\n",
    "main_logger.info(f\"  - Translators: {len(set_data_translators)}\")\n",
    "main_logger.info(f\"  - Languages covered:\")\n",
    "main_logger.info(f\"    - Source languages: {len(dict_source_langs)}\")\n",
    "main_logger.info(f\"    - Target languages: {len(dict_target_langs)}\")\n",
    "main_logger.info(f\"  - Manufacturers/clients: {len(set_manufacturers)}\")\n",
    "\n",
    "\n",
    "# Dataframe of final schema\n",
    "final_schema_df = pd.DataFrame({\n",
    "    \"Column\": df_final.columns,\n",
    "    \"Type\": df_final.dtypes,\n",
    "    \"Non-Null Count\": df_final.count(),\n",
    "    \"Null Count\": df_final.isnull().sum(),\n",
    "    \"Null Percentage\": (df_final.isnull().sum() / len(df_final) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Log the final schema using the main logger\n",
    "main_logger.info(f\"\\nFinal Schema: \\n\\n{final_schema_df.to_string()}\\n\")\n",
    "main_logger.info(\"DATA MERGE PROCESS COMPLETED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final columns\n",
    "print(\"Final Columns:\")\n",
    "for col in df_final.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "display(df_final.head())  # Display Merged DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
