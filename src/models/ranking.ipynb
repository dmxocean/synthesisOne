{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {},
      "source": [
        "# Ranking Model Training and Evaluation\n",
        "\n",
        "This notebook trains and evaluates the LGBMRanker model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a430de",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.disable(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH_NOTEBOOK = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "PATH_SRC = os.path.dirname(PATH_NOTEBOOK)\n",
        "PATH_ROOT = os.path.dirname(PATH_SRC)\n",
        "sys.path.append(PATH_ROOT)\n",
        "\n",
        "# Path constants\n",
        "PATH_DATA_PROCESSED = os.path.join(PATH_ROOT, \"data\", \"processed\")\n",
        "PATH_RANKING_PROCESSED = os.path.join(PATH_DATA_PROCESSED, \"ranking\")\n",
        "PATH_CONFIG = os.path.join(PATH_ROOT, \"config\", \"ranking_config.yaml\")\n",
        "\n",
        "PATH_BASE_PROCESSED = os.path.join(PATH_DATA_PROCESSED, \"base\")\n",
        "PATH_BASE_ARTIFACTS = os.path.join(PATH_BASE_PROCESSED, \"artifacts\")\n",
        "\n",
        "PATH_MODEL_DIR = os.path.join(PATH_ROOT, \"models\", \"ranking\")\n",
        "PATH_RESULTS_DIR = os.path.join(PATH_ROOT, \"results\", \"ranking\")\n",
        "PATH_PLOTS_DIR = os.path.join(PATH_RESULTS_DIR, \"plots\")\n",
        "PATH_GRID_SEARCH_DIR = os.path.join(PATH_RESULTS_DIR, \"grid_search\")\n",
        "\n",
        "for dir_path in [PATH_MODEL_DIR, PATH_RESULTS_DIR, PATH_PLOTS_DIR, PATH_GRID_SEARCH_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models.ranking import (\n",
        "    load_config, load_artifacts, load_data, train_lgbm_ranker,\n",
        "    evaluate_lgbm_ranker, save_model_and_metrics, perform_grid_search,\n",
        "    PARAM_GRIDS\n",
        ")\n",
        "\n",
        "from src.evaluation.visualization import (\n",
        "    generate_feature_importance_plot, generate_learning_curve_plot,\n",
        "    generate_ndcg_curve_plot, generate_visualizations, visualize_grid_search_results\n",
        ")\n",
        "\n",
        "from src.evaluation.analysis import (\n",
        "    analyze_feature_importance, analyze_performance_by_group_size\n",
        ")\n",
        "\n",
        "from src.evaluation.comparison import compare_with_random_baseline\n",
        "\n",
        "from src.prediction.ranking import run_inference\n",
        "\n",
        "from src.utils.utils import setup_logger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {},
      "source": [
        "## Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = load_config()\n",
        "print(\"Configuration loaded with parameters:\", json.dumps(config[\"parameters\"], indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    (X_train, groups_train, relevance_train, \n",
        "     X_val, groups_val, relevance_val, \n",
        "     X_test, groups_test, relevance_test,\n",
        "     task_ids_train, task_ids_val, task_ids_test) = load_data()\n",
        "    \n",
        "    # Display sample of the training data\n",
        "    print(\"\\nSample training data features:\")\n",
        "    print(X_train.head().to_string(index=False))\n",
        "    \n",
        "    # Display relevance score distribution\n",
        "    print(\"\\nRelevance score distribution:\")\n",
        "    unique_relevance, counts = np.unique(relevance_train, return_counts=True)\n",
        "    for rel, count in zip(unique_relevance, counts):\n",
        "        print(f\"  Relevance {rel}: {count} samples ({count/len(relevance_train)*100:.2f}%)\")\n",
        "    \n",
        "    # Display feature names\n",
        "    print(\"\\nFeatures available for training:\")\n",
        "    for i, feature in enumerate(X_train.columns, 1):\n",
        "        print(f\"  {i}. {feature}\")\n",
        "    \n",
        "    data_loaded = True\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {str(e)}\")\n",
        "    data_loaded = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {},
      "source": [
        "## Grid Search Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toggle grid search on/off\n",
        "PERFORM_GRID_SEARCH = False  # Set to True to run grid search\n",
        "\n",
        "# Choose grid size\n",
        "GRID_SIZE = \"M\"  # Options: \"S\", \"M\", \"L\"\n",
        "\n",
        "# Grid search implementation\n",
        "best_gs_params = None\n",
        "\n",
        "if PERFORM_GRID_SEARCH and data_loaded:\n",
        "    try:\n",
        "        print(f\"Starting grid search with {GRID_SIZE} parameter grid...\")\n",
        "        \n",
        "        active_param_grid = PARAM_GRIDS[GRID_SIZE]\n",
        "        \n",
        "        # Calculate total combinations\n",
        "        total_combinations = np.prod([len(v) for v in active_param_grid.values()])\n",
        "        print(f\"Using {GRID_SIZE} grid with {total_combinations} combinations\")\n",
        "        \n",
        "        # Get base parameters from config\n",
        "        base_params = {k: v for k, v in config[\"parameters\"].items() if k not in active_param_grid}\n",
        "        \n",
        "        # Run grid search\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        best_gs_model, best_gs_params, grid_results = perform_grid_search(\n",
        "            X_train, relevance_train, groups_train,\n",
        "            X_val, relevance_val, groups_val,\n",
        "            active_param_grid, base_params,\n",
        "            num_boost_round=config['training']['num_boost_round'],\n",
        "            early_stopping_rounds=config['training']['early_stopping_rounds']\n",
        "        )\n",
        "        \n",
        "        # Save results\n",
        "        grid_results.to_csv(os.path.join(PATH_GRID_SEARCH_DIR, f\"grid_search_results_{timestamp}.csv\"), index=False)\n",
        "        \n",
        "        # Visualize results\n",
        "        visualize_grid_search_results(grid_results, active_param_grid)\n",
        "        \n",
        "        print(\"\\nGrid search completed successfully!\")\n",
        "        print(f\"Best parameters: {best_gs_params}\")\n",
        "        print(f\"Best validation combined score: {grid_results['combined_score'].max():.4f}\")\n",
        "        \n",
        "        # Update config with best parameters\n",
        "        config[\"parameters\"].update(best_gs_params)\n",
        "        print(\"\\nConfiguration updated with best parameters from grid search\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during grid search: {str(e)}\")\n",
        "else:\n",
        "    if not data_loaded:\n",
        "        print(\"Data not loaded\")\n",
        "    elif not PERFORM_GRID_SEARCH:\n",
        "        print(\"Grid search is disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {},
      "outputs": [],
      "source": [
        "if data_loaded:\n",
        "    try:\n",
        "        print(\"Starting model training...\")\n",
        "        \n",
        "        # Train model\n",
        "        model, training_history = train_lgbm_ranker(\n",
        "            X_train, relevance_train, groups_train,\n",
        "            X_val, relevance_val, groups_val,\n",
        "            config\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nModel training completed successfully\")\n",
        "        print(f\"Best iteration: {model.best_iteration}\")\n",
        "        \n",
        "        model_trained = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        model_trained = False\n",
        "else:\n",
        "    print(\"Cannot train model as data loading failed\")\n",
        "    model_trained = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_trained:\n",
        "    try:\n",
        "        print(\"Evaluating model on test data...\")\n",
        "        \n",
        "        # Evaluate model\n",
        "        metrics = evaluate_lgbm_ranker(\n",
        "            model, X_test, relevance_test, groups_test, config\n",
        "        )\n",
        "\n",
        "        print(\"\\nEvaluation metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        evaluation_complete = True\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {str(e)}\")\n",
        "        evaluation_complete = False\n",
        "else:\n",
        "    print(\"Cannot evaluate model as training failed\")\n",
        "    evaluation_complete = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {},
      "source": [
        "## Save Model and Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16",
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_trained and evaluation_complete:\n",
        "    try:\n",
        "        print(\"Saving model and generating visualizations...\")\n",
        "        \n",
        "        # Save model and metrics\n",
        "        save_model_and_metrics(\n",
        "            model, training_history, metrics, X_train.columns.tolist(), config\n",
        "        )\n",
        "        \n",
        "        # Generate visualizations\n",
        "        generate_visualizations(\n",
        "            model, training_history, metrics, X_train.columns.tolist()\n",
        "        )\n",
        "        \n",
        "        print(\"\\nModel saved to:\")\n",
        "        print(f\"  - {os.path.join(PATH_MODEL_DIR, 'lgbm_ranker_model.pkl')}\")\n",
        "        print(f\"  - {os.path.join(PATH_MODEL_DIR, 'lgbm_ranker_model.txt')}\")\n",
        "        \n",
        "        print(\"\\nResults saved to:\")\n",
        "        print(f\"  - Metrics: {os.path.join(PATH_RESULTS_DIR, 'metrics.json')}\")\n",
        "        print(f\"  - Feature importance: {os.path.join(PATH_PLOTS_DIR, 'feature_importance.png')}\")\n",
        "        print(f\"  - Learning curve: {os.path.join(PATH_PLOTS_DIR, 'learning_curve.png')}\")\n",
        "        print(f\"  - NDCG curve: {os.path.join(PATH_PLOTS_DIR, 'ndcg_curve.png')}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model and visualizations: {str(e)}\")\n",
        "else:\n",
        "    print(\"Cannot save model as training or evaluation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {},
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_trained and evaluation_complete:\n",
        "    analyze_performance_by_group_size(model, X_test, relevance_test, groups_test)\n",
        "else:\n",
        "    print(\"Model or test data not available. Please train and evaluate the model first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {},
      "source": [
        "## Compare with Random Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_with_random_baseline(model, X_test, relevance_test, groups_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {},
      "source": [
        "## Model Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample tasks for inference\n",
        "sample_tasks = pd.DataFrame({\n",
        "    'PROJECT_ID': [220901, 220901, 220902],\n",
        "    'PM': ['JDM', 'JDM', 'SAB'],\n",
        "    'TASK_ID': [11230500, 11230501, 11230502],\n",
        "    'START': ['2025-05-20 09:00:00', '2025-05-20 14:00:00', '2025-05-21 10:00:00'],\n",
        "    'END': ['2025-05-22 17:00:00', '2025-05-21 18:00:00', '2025-05-23 15:00:00'],\n",
        "    'TASK_TYPE': ['Translation', 'Proofreading', 'Engineering'],\n",
        "    'SOURCE_LANG': ['English', 'German', 'English'],\n",
        "    'TARGET_LANG': ['Spanish (Iberian)', 'French', 'German'],\n",
        "    'FORECAST': [3.5, 2.0, 5.0],\n",
        "    'COST': [59.5, 40.0, 85.0],\n",
        "    'MANUFACTURER': ['Respiro Dynamics', 'SunTech', 'Biomedical Inc'],\n",
        "    'MANUFACTURER_SECTOR': ['Information Technology', 'Health Care', 'Health Care']\n",
        "})\n",
        "\n",
        "sample_tasks['START'] = pd.to_datetime(sample_tasks['START'])\n",
        "sample_tasks['END'] = pd.to_datetime(sample_tasks['END'])\n",
        "\n",
        "print(\"Sample tasks for inference (with all required columns):\")\n",
        "print(sample_tasks.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27",
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_trained:\n",
        "    print(\"\\nRunning inference to get actual translator predictions...\")\n",
        "    try:\n",
        "        # Run inference to get translator recommendations\n",
        "        translator_recommendations = run_inference(sample_tasks, top_k=10)\n",
        "        \n",
        "        # Display detailed recommendations for each task\n",
        "        print(\"\\nActual Translator Predictions:\")\n",
        "        for task_id, recommendations in translator_recommendations.items():\n",
        "            # Get task details\n",
        "            task_info = sample_tasks[sample_tasks['TASK_ID'] == task_id].iloc[0]\n",
        "            \n",
        "            print(f\"Task ID: {task_id}\")\n",
        "            print(f\"Project: {task_info['PROJECT_ID']} (PM: {task_info['PM']})\")\n",
        "            print(f\"Type: {task_info['TASK_TYPE']}\")\n",
        "            print(f\"Languages: {task_info['SOURCE_LANG']} -> {task_info['TARGET_LANG']}\")\n",
        "            print(f\"Duration: {task_info['FORECAST']} hours\")\n",
        "            print(f\"Cost: ${task_info['COST']}\")\n",
        "            print(f\"Client: {task_info['MANUFACTURER']} ({task_info['MANUFACTURER_SECTOR']})\")\n",
        "            \n",
        "            print(f\"\\nTop 10 Recommended Translators:\")\n",
        "            print(f\"{'Rank':<6}{'Translator ID':<15}{'Score':<12}\")\n",
        "            \n",
        "            for rank, (translator, score) in enumerate(recommendations, 1):\n",
        "                print(f\"{rank:<6}{translator:<15}{score:<12.4f}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {str(e)}\")\n",
        "else:\n",
        "    print(\"\\nModel not available for inference. Please train the model first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {},
      "source": [
        "## Batch Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a batch of tasks\n",
        "batch_tasks_data = [\n",
        "    [220901, 'JDM', 11230500, '2025-05-20 09:00:00', '2025-05-22 17:00:00', 'Translation', 'English', 'Spanish (Iberian)', 3.5, 59.5, 'Respiro Dynamics', 'Information Technology'],\n",
        "    [220901, 'JDM', 11230501, '2025-05-20 14:00:00', '2025-05-21 18:00:00', 'Proofreading', 'German', 'French', 2.0, 40.0, 'SunTech', 'Health Care'],\n",
        "    [220902, 'SAB', 11230502, '2025-05-21 10:00:00', '2025-05-23 15:00:00', 'Engineering', 'English', 'German', 5.0, 85.0, 'Biomedical Inc', 'Health Care'],\n",
        "    [220902, 'SAB', 11230503, '2025-05-21 13:00:00', '2025-05-22 17:00:00', 'Review', 'Spanish (Iberian)', 'English', 1.5, 30.0, 'TechCorp', 'Technology'],\n",
        "    [220903, 'KMT', 11230504, '2025-05-22 08:00:00', '2025-05-24 17:00:00', 'Translation', 'French', 'Spanish (Iberian)', 4.0, 68.0, 'MedTech Solutions', 'Health Care'],\n",
        "    [220903, 'KMT', 11230505, '2025-05-22 09:00:00', '2025-05-23 12:00:00', 'Proofreading', 'English', 'French', 2.5, 42.5, 'InnovateWorks', 'Consumer Discretionary'],\n",
        "    [220904, 'RPN', 11230506, '2025-05-23 10:00:00', '2025-05-25 16:00:00', 'Translation', 'Italian', 'English', 3.0, 51.0, 'AutoParts GmbH', 'Manufacturing'],\n",
        "    [220904, 'RPN', 11230507, '2025-05-23 14:00:00', '2025-05-24 18:00:00', 'Review', 'English', 'Spanish (Iberian)', 1.0, 17.0, 'FinanceGroup', 'Financials'],\n",
        "    [220905, 'JDM', 11230508, '2025-05-24 09:00:00', '2025-05-26 17:00:00', 'Engineering', 'German', 'Italian', 6.0, 102.0, 'Pharma Ltd', 'Health Care'],\n",
        "    [220905, 'JDM', 11230509, '2025-05-24 15:00:00', '2025-05-25 17:00:00', 'Proofreading', 'Spanish (Iberian)', 'German', 2.0, 34.0, 'ConsumerGoods Co', 'Consumer Staples']\n",
        "]\n",
        "\n",
        "columns = ['PROJECT_ID', 'PM', 'TASK_ID', 'START', 'END', 'TASK_TYPE', \n",
        "           'SOURCE_LANG', 'TARGET_LANG', 'FORECAST', 'COST', \n",
        "           'MANUFACTURER', 'MANUFACTURER_SECTOR']\n",
        "\n",
        "batch_tasks = pd.DataFrame(batch_tasks_data, columns=columns)\n",
        "\n",
        "batch_tasks['START'] = pd.to_datetime(batch_tasks['START'])\n",
        "batch_tasks['END'] = pd.to_datetime(batch_tasks['END'])\n",
        "\n",
        "print(\"Batch of tasks for inference (production format):\")\n",
        "print(batch_tasks.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30",
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_trained:\n",
        "    print(\"\\nRunning batch inference for all tasks...\")\n",
        "    try:\n",
        "        # Run batch inference\n",
        "        batch_recommendations = run_inference(batch_tasks, top_k=5)\n",
        "        \n",
        "        print(\"\\nBatch Inference Results Summary:\")\n",
        "        \n",
        "        results_summary = []\n",
        "        \n",
        "        for index, row in batch_tasks.iterrows():\n",
        "            task_id = row['TASK_ID']\n",
        "            \n",
        "            if task_id in batch_recommendations:\n",
        "                recommendations = batch_recommendations[task_id]\n",
        "                top_translator = recommendations[0][0] if recommendations else 'None'\n",
        "                top_score = recommendations[0][1] if recommendations else 0.0\n",
        "                \n",
        "                summary_row = {\n",
        "                    'Task_ID': task_id,\n",
        "                    'Project': row['PROJECT_ID'],\n",
        "                    'PM': row['PM'],\n",
        "                    'Type': row['TASK_TYPE'],\n",
        "                    'Languages': f\"{row['SOURCE_LANG'][:3]}->{row['TARGET_LANG'][:3]}\",\n",
        "                    'Top_Translator': top_translator,\n",
        "                    'Score': f\"{top_score:.3f}\",\n",
        "                    'Num_Candidates': len(recommendations)\n",
        "                }\n",
        "                results_summary.append(summary_row)\n",
        "        \n",
        "        summary_df = pd.DataFrame(results_summary)\n",
        "        print(summary_df.to_string(index=False))\n",
        "        \n",
        "        # Show predictions\n",
        "        print(\"\\nDetailed Predictions :\")\n",
        "        for i, (task_id, recommendations) in enumerate(batch_recommendations.items()):\n",
        "            if i >= 3:  # Only show some examples\n",
        "                break\n",
        "                \n",
        "            task_info = batch_tasks[batch_tasks['TASK_ID'] == task_id].iloc[0]\n",
        "            print(f\"\\n{'-'*40}\")\n",
        "            print(f\"Task {task_id}: {task_info['SOURCE_LANG']} > {task_info['TARGET_LANG']} ({task_info['TASK_TYPE']})\")\n",
        "            print(\"Recommended Translators:\")\n",
        "            \n",
        "            for rank, (translator, score) in enumerate(recommendations[:5], 1):\n",
        "                print(f\"  {rank}. {translator} (score: {score:.4f})\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error during batch inference: {str(e)}\")\n",
        "else:\n",
        "    print(\"\\nModel not available for batch inference. Please train the model first\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synthesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
