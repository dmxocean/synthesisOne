{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import re\n",
                "\n",
                "import time\n",
                "import datetime\n",
                "import difflib\n",
                "import warnings\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from matplotlib import cm \n",
                "from collections import Counter\n",
                "from sklearn.cluster import KMeans\n",
                "from scipy import stats\n",
                "\n",
                "sns.reset_orig()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_base = os.path.dirname((os.getcwd())) # Base path of the repository\n",
                "sys.path.append(path_base) # Add base path to system path\n",
                "\n",
                "from utils.time import time_difference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Pandas display options\n",
                "# pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
                "# pd.set_option(\"display.max_columns\", None)\n",
                "# pd.set_option(\"display.max_rows\", 50)\n",
                "# pd.set_option(\"display.width\", 1000)\n",
                "\n",
                "# # First reset to default matplotlib style\n",
                "# plt.rcdefaults()\n",
                "\n",
                "# # Define a consistent color palette (using matplotlib default colors)\n",
                "# colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
                "\n",
                "# Default figure parameters\n",
                "plt.rcParams.update({\n",
                "    \"figure.figsize\": (12, 6),\n",
                "    \"figure.dpi\": 300,\n",
                "    \"figure.autolayout\": True,\n",
                "    # Font properties\n",
                "    \"font.size\": 11,\n",
                "    \"axes.titlesize\": 12,\n",
                "    \"axes.labelsize\": 12,\n",
                "    \"xtick.labelsize\": 11,\n",
                "    \"ytick.labelsize\": 11,\n",
                "    \"legend.fontsize\": 11,\n",
                "    # Axes appearance\n",
                "    # \"axes.grid\": True,\n",
                "    \"grid.alpha\": 0.5,\n",
                "    # \"grid.linestyle\": \"--\",\n",
                "    # Edge color and linewidth for patches\n",
                "    \"patch.edgecolor\": \"black\",\n",
                "    \"patch.linewidth\": 0.75,\n",
                "    \"axes.linewidth\": 1.0,\n",
                "})\n",
                "\n",
                "# Seaborn style settings\n",
                "sns.set_style({\"patch.edgecolor\": \"black\", \"patch.linewidth\": 0.75})\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_data = os.path.join(path_base, \"data\", \"raw\") # Path to the CSV files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Status columns to be parsed as datetime\n",
                "status_columns = [\"ASSIGNED\", \"READY\", \"WORKING\", \"DELIVERED\", \"RECEIVED\", \"CLOSE\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load CSVs\n",
                "try: \n",
                "    df_clients = pd.read_csv(os.path.join(path_data, \"clients.csv\"))\n",
                "    df_data = pd.read_csv(os.path.join(path_data, \"data.csv\"), dtype={\"PROJECT_ID\": \"object\"}, parse_dates=status_columns)\n",
                "    df_schedules = pd.read_csv(os.path.join(path_data, \"schedules.csv\"))\n",
                "    df_translators = pd.read_csv(os.path.join(path_data, \"translatorsCostPairs.csv\"))\n",
                "except FileNotFoundError as e:\n",
                "    print(f\"File not found: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuples_df = {\n",
                "    \"Clients\": df_clients,\n",
                "    \"Sample\": df_data,\n",
                "    \"Schedules\": df_schedules,\n",
                "    \"Translators\": df_translators\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"MISSING VALUES\\n\")\n",
                "\n",
                "for name, df in tuples_df.items():\n",
                "    print(f\"{name} Dataset\")\n",
                "    print(f\"{df.isnull().sum().sum()} missing values\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"DUPLICATED VALUES\\n\")\n",
                "\n",
                "for name, df in tuples_df.items():\n",
                "    print(f\"{name} Dataset\")\n",
                "    print(f\"  {df.duplicated().sum()} duplicated value\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"UNIQUE VALUES\\n\")\n",
                "\n",
                "for name, df in tuples_df.items():\n",
                "    print(f\"{name} Dataset\")\n",
                "    print(f\"  Unique Values:\")\n",
                "    for col in df.columns:\n",
                "        print(f\"    {col}: {df[col].nunique()}\")    \n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"DATASETS\\n\")\n",
                "\n",
                "for name, df in tuples_df.items():\n",
                "    print(f\"{name} Dataset\")\n",
                "    print(f\"  {df.shape[0]} rows and {df.shape[1]} columns\")\n",
                "    print(f\"  Data Types:\")\n",
                "    for col, dtype in df.dtypes.items():\n",
                "        print(f\"    {col}: {dtype}n\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_unique_values(df):\n",
                "    \"\"\"\n",
                "    Get unique values for each column in a DataFrame\n",
                "\n",
                "    Parameters:\n",
                "        df (pd.DataFrame): DataFrame to get unique values from\n",
                "\n",
                "    Returns:\n",
                "        dict: Unique values for each column\n",
                "    \"\"\"\n",
                "\n",
                "    unique_values = {}\n",
                "    for col in df.columns:\n",
                "        unique_values[col] = df[col].unique()\n",
                "    return unique_values\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cols_useful = {\n",
                "    \"Clients\": [\"CLIENT_NAME\", \"WILDCARD\"],\n",
                "    \"Sample\": [\n",
                "        \"TASK_TYPE\",\n",
                "        \"SOURCE_LANG\",\n",
                "        \"TARGET_LANG\",\n",
                "        \"TRANSLATOR\",\n",
                "        \"MANUFACTURER\",\n",
                "        \"MANUFACTURER_SECTOR\",\n",
                "        \"MANUFACTURER_INDUSTRY_GROUP\",\n",
                "        \"MANUFACTURER_INDUSTRY\",\n",
                "        \"MANUFACTURER_SUBINDUSTRY\",\n",
                "    ],\n",
                "    \"Schedules\": [\"NAME\"],\n",
                "    \"Translators\": [\"TRANSLATOR\", \"SOURCE_LANG\", \"TARGET_LANG\", \"HOURLY_RATE\"],\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"UNIQUE VALUES\\n\")\n",
                "\n",
                "# Unique values for each column in each DataFrame\n",
                "unique_values = {}\n",
                "\n",
                "for name, df in tuples_df.items():\n",
                "    unique_values[name] = get_unique_values(df[cols_useful[name]])   \n",
                "\n",
                "for name, values in unique_values.items():\n",
                "    print(f\"{name} Dataset\")\n",
                "    for col, unique_vals in values.items():\n",
                "        display(pd.DataFrame({col: unique_vals}))\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### CSV Clients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Number of columns: {df_clients.shape[1]}\")\n",
                "print(f\"Number of unique clients: {df_clients['CLIENT_NAME'].nunique()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Sample data:\")\n",
                "display(df_clients.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics with additional metrics skewness and kurtosis\n",
                "print(\"Detailed summary statistics:\")\n",
                "numeric_cols = df_clients.select_dtypes(include=[\"number\"])  # Select only numeric columns\n",
                "client_stats = numeric_cols.describe()\n",
                "client_stats.loc[\"skew\"] = numeric_cols.skew()\n",
                "client_stats.loc[\"kurtosis\"] = numeric_cols.kurtosis()\n",
                "\n",
                "display(client_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Number of unique clients\n",
                "print(f\"Number of unique clients: {df_clients['CLIENT_NAME'].nunique()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for duplicates in client names\n",
                "duplicate_clients = df_clients[df_clients.duplicated(subset=[\"CLIENT_NAME\"], keep=False)]\n",
                "print(f\"Number of duplicate client names: {len(duplicate_clients)}\")\n",
                "if len(duplicate_clients) > 0:\n",
                "    print(\"Duplicate clients found:\")\n",
                "    display(duplicate_clients)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of selling hourly prices\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_clients[\"SELLING_HOURLY_PRICE\"], kde=True, bins=20, alpha=0.75)\n",
                "plt.title(\"Distribution of Selling Hourly Prices\")\n",
                "plt.xlabel(\"Price\")\n",
                "plt.ylabel(\"Count Clients\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Distribution of Minimum Quality requirements\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_clients[\"MIN_QUALITY\"], kde=True, bins=20, alpha=0.75)\n",
                "plt.title(\"Distribution of Minimum Quality Requirements\")\n",
                "plt.xlabel(\"Minimum Quality\")\n",
                "plt.ylabel(\"Count Clients\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze relationship between selling price and Minimum Quality\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.scatterplot(data=df_clients, x=\"MIN_QUALITY\", y=\"SELLING_HOURLY_PRICE\")\n",
                "plt.title(\"Relationship between Minimum Quality and Selling Price\")\n",
                "plt.xlabel(\"Minimum Quality\")\n",
                "plt.ylabel(\"Selling Hourly Price\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Average selling price by Minimum Quality\n",
                "avg_price_by_quality = df_clients.groupby(\"MIN_QUALITY\")[\"SELLING_HOURLY_PRICE\"].mean().reset_index()\n",
                "\n",
                "print(\"Average selling price by Minimum Quality requirement:\")\n",
                "for quality in avg_price_by_quality[\"MIN_QUALITY\"]:\n",
                "    print(f\"Quality {quality:.0f}: {avg_price_by_quality[avg_price_by_quality['MIN_QUALITY'] == quality]['SELLING_HOURLY_PRICE'].values[0]:.2f}\")\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.barplot(data=avg_price_by_quality, x=\"MIN_QUALITY\", y=\"SELLING_HOURLY_PRICE\", alpha=0.75)\n",
                "plt.title(\"Average Selling Price by Minimum Quality Requirement\")\n",
                "plt.xlabel(\"Minimum Quality Requirement\")\n",
                "plt.ylabel(\"Average Selling Hourly Price\")\n",
                "\n",
                "# Add count labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    ax.text(\n",
                "        p.get_x() + p.get_width() / 2,\n",
                "        p.get_height(),\n",
                "        f\"{avg_price_by_quality['SELLING_HOURLY_PRICE'].iloc[i]:.2f}\",\n",
                "        ha=\"center\",\n",
                "        va=\"bottom\",\n",
                "    )\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# WILDCARD analysis\n",
                "wildcard_counts = df_clients[\"WILDCARD\"].value_counts()\n",
                "\n",
                "# Frquency of WILDCARD values\n",
                "print(\"Frequency of WILDCARD values:\")\n",
                "for wildcard, count in wildcard_counts.items():\n",
                "    print(f\"WILDCARD {wildcard}: {count} occurrences\")\n",
                "\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.bar(wildcard_counts.index, wildcard_counts.values, alpha=0.75)\n",
                "plt.title(\"Frequency of WILDCARD Values\")\n",
                "plt.xlabel(\"WILDCARD Value\")\n",
                "plt.ylabel(\"Count\")\n",
                "\n",
                "# Add count labels\n",
                "for i, count in enumerate(wildcard_counts.values):\n",
                "    plt.text(i, count + 0.5, str(count), ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "price_quantiles = df_clients[\"SELLING_HOURLY_PRICE\"].quantile([0, 0.25, 0.5, 0.75, 1]).values # Define Pricing Tiers based on Quantiles\n",
                "price_quantiles = pd.unique(price_quantiles) # Avoid duplicates in quantiles\n",
                "pricing_tier_labels = [\"Budget\", \"Economy\", \"Standard\", \"Premium\", \"Enterprise\"]\n",
                "\n",
                "pricing_tiers = pd.cut( # Assign Pricing Tiers based on hourly selling price\n",
                "    df_clients[\"SELLING_HOURLY_PRICE\"], \n",
                "    bins=price_quantiles, \n",
                "    labels=pricing_tier_labels[:len(price_quantiles)-1],  # Adjust labels to match unique bins\n",
                "    include_lowest=True\n",
                ")\n",
                "df_clients[\"PRICING_TIER\"] = pricing_tiers\n",
                "\n",
                "# Calculate Pricing Tier distribution\n",
                "tier_counts = df_clients[\"PRICING_TIER\"].value_counts().sort_index()\n",
                "tier_percentages = tier_counts / len(df_clients) * 100\n",
                "# print(tier_counts)\n",
                "\n",
                "print(\"Pricing Tier distribution:\")\n",
                "for tier, count in tier_counts.items():\n",
                "    percentage = tier_percentages[tier]\n",
                "    price_range = f\"{price_quantiles[pricing_tier_labels.index(tier)]:.2f} - {price_quantiles[pricing_tier_labels.index(tier) + 1]:.2f}\"\n",
                "    print(f\"  {tier} ({price_range}): {count} clients ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize Pricing Tiers\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.bar(tier_counts.index, tier_counts.values, alpha=0.75)\n",
                "plt.title(\"Client distribution by Pricing Tier\")\n",
                "plt.xlabel(\"Pricing Tier\")\n",
                "plt.ylabel(\"Number Clients\")\n",
                "\n",
                "# Add count labels\n",
                "for i, count in enumerate(tier_counts.values):\n",
                "    plt.text(i, count + 0.5, str(count), ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quality Tiers\n",
                "quality_tier_bounds = [0, 6, 7, 8, 9, 10]\n",
                "quality_tier_labels = [\"Minimal\", \"Basic\", \"Standard\", \"High\", \"Premium\"]\n",
                "\n",
                "# Assign Quality Tiers based on Minimum Quality requirement\n",
                "quality_tiers = pd.cut(\n",
                "    df_clients[\"MIN_QUALITY\"], \n",
                "    bins=quality_tier_bounds, \n",
                "    labels=quality_tier_labels,\n",
                "    include_lowest=True\n",
                ")\n",
                "\n",
                "df_clients[\"QUALITY_TIER\"] = quality_tiers\n",
                "\n",
                "quality_tier_counts = df_clients[\"QUALITY_TIER\"].value_counts().sort_index()\n",
                "quality_tier_counts = quality_tier_counts[quality_tier_counts > 0]  # Remove tiers with no clients\n",
                "quality_tier_percentages = quality_tier_counts / len(df_clients) * 100\n",
                "\n",
                "print(\"Quality Tier distribution:\")\n",
                "for tier, count in quality_tier_counts.items():\n",
                "    percentage = quality_tier_percentages[tier]\n",
                "    tier_index = quality_tier_labels.index(tier)\n",
                "    quality_range = f\"{quality_tier_bounds[tier_index]} - {quality_tier_bounds[tier_index + 1]}\"\n",
                "    print(f\"  {tier} ({quality_range}): {count} clients ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize Quality Tiers\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.bar(quality_tier_counts.index, quality_tier_counts.values, alpha=0.75)\n",
                "plt.title(\"Client Distribution by Quality Requirement Tier\")\n",
                "plt.xlabel(\"Quality Tier\")\n",
                "plt.ylabel(\"Number Clients\")\n",
                "\n",
                "# Add count labels\n",
                "for i, count in enumerate(quality_tier_counts.values):\n",
                "    plt.text(i, count + 0.5, str(count), ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-tabulation of price tiers vs. Quality Tiers\n",
                "price_quality_crosstab = pd.crosstab(\n",
                "    df_clients[\"PRICING_TIER\"], \n",
                "    df_clients[\"QUALITY_TIER\"],\n",
                "    normalize=\"index\"\n",
                ") * 100\n",
                "\n",
                "print(\"Relationship between Pricing and Quality Tiers:\")\n",
                "print(price_quality_crosstab)\n",
                "\n",
                "# Create a heatmap for the cross-tabulation\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.heatmap(price_quality_crosstab, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
                "plt.title(\"Pricing Tier vs Quality Tier (% within Pricing Tier)\")\n",
                "plt.ylabel(\"Pricing Tier\")\n",
                "plt.xlabel(\"Quality Tier\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Think about this as:\n",
                "# - Percentage distribution of Quality Tiers within each Pricing Tier [Minimal | Basic | Standard] highlighting relationships between client pricing and quality preferences\"\n",
                "# - e.g, 41.2% of \"Economy\" clients have \"Standard\" quality requirements\n",
                "# - e.g, 39.7% of \"Budget\" clients have \"Basic\" quality requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# How much a client pays per unit of their Minimum Quality requirement\n",
                "# - Low value: They pay less per quality point, potentially a \"better deal\"\n",
                "# - High value: They pay more per quality point, higher cost for the same quality level \n",
                "\n",
                "# Price per quality point\n",
                "df_clients[\"PRICE_PER_QUALITY\"] = df_clients[\"SELLING_HOURLY_PRICE\"] / df_clients[\"MIN_QUALITY\"]\n",
                "\n",
                "# Display price per quality point statistics\n",
                "print(\"Price per Quality Point statistics:\")\n",
                "price_per_quality_stats = df_clients[~df_clients[\"PRICE_PER_QUALITY\"].isin([np.inf])][\"PRICE_PER_QUALITY\"].describe()\n",
                "print(price_per_quality_stats)\n",
                "\n",
                "# Visualize price per quality distribution\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_clients[\"PRICE_PER_QUALITY\"], kde=True, bins=25, alpha=0.75)\n",
                "plt.title(\"Distribution of Price per Quality Point\")\n",
                "plt.xlabel(\"Price per Quality Point\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.axvline(price_per_quality_stats[\"mean\"], color=\"blue\", linestyle=\"--\", label=f\"Mean: {price_per_quality_stats['mean']:.2f}\")\n",
                "plt.axvline(price_per_quality_stats[\"50%\"], color=\"red\", linestyle=\"--\", label=f\"Median: {price_per_quality_stats['50%']:.2f}\")\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between WILDCARD and selling price\n",
                "print(\"Average selling price by WILDCARD:\")\n",
                "print(df_clients.groupby(\"WILDCARD\")[\"SELLING_HOURLY_PRICE\"].mean())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between WILDCARD and Minimum Quality\n",
                "print(\"Average Minimum Quality by WILDCARD:\")\n",
                "print(df_clients.groupby(\"WILDCARD\")[\"MIN_QUALITY\"].mean())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze WILDCARD field more deeply\n",
                "print(\"WILDCARD representation:\")\n",
                "wildcard_counts = df_clients[\"WILDCARD\"].value_counts()\n",
                "wildcard_percentage = wildcard_counts / len(df_clients) * 100\n",
                "\n",
                "for wildcard, count in wildcard_counts.items():\n",
                "    percentage = wildcard_percentage[wildcard]\n",
                "    print(f\"  {wildcard}: {count} clients ({percentage:.2f}%)\")\n",
                "\n",
                "# Cross-tabulate WILDCARD vs Quality Tier\n",
                "wildcard_quality_crosstab = pd.crosstab(\n",
                "    df_clients[\"WILDCARD\"], \n",
                "    df_clients[\"QUALITY_TIER\"],\n",
                "    normalize=\"index\"\n",
                ") * 100\n",
                "\n",
                "# Cross-tabulate WILDCARD vs Pricing Tier\n",
                "wildcard_price_crosstab = pd.crosstab(\n",
                "    df_clients[\"WILDCARD\"], \n",
                "    df_clients[\"PRICING_TIER\"],\n",
                "    normalize=\"index\"\n",
                ") * 100\n",
                "\n",
                "print(\"\\nRelationship between WILDCARD and Quality Tier:\")\n",
                "# Visualize relationship between WILDCARD and Quality Tier\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.heatmap(wildcard_quality_crosstab, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
                "plt.title(\"WILDCARD vs Quality Tier\")\n",
                "plt.ylabel(\"WILDCARD\")\n",
                "plt.xlabel(\"Quality Tier\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nRelationship between WILDCARD and Pricing Tier:\")\n",
                "# Visualize relationship between WILDCARD and Pricing Tier\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.heatmap(wildcard_price_crosstab, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
                "plt.title(\"WILDCARD vs Pricing Tier\")\n",
                "plt.ylabel(\"WILDCARD\")\n",
                "plt.xlabel(\"Pricing Tier\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "\n",
                "# Think about this as:\n",
                "# - Percentage distribution of quality/Pricing Tiers within each WILDCARD, highlighting relationships between client preferences and quality/pricing\n",
                "# - e.g., 31.9% of \"Deadline\" WILDCARD clients have \"Minimal\" quality requirements\n",
                "# - e.g., 39.7% of \"Deadline\" WILDCARD clients have \"Budget\" pricing requirements"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Clients Segments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analysis of clients based on price and quality\n",
                "# - Use KMeans clustering to group clients based on selling price and Minimum Quality requirements\n",
                "# - Analyze the clusters to identify different client segments\n",
                "\n",
                "# Prepare data for clustering\n",
                "cluster_data = df_clients[[\"SELLING_HOURLY_PRICE\", \"MIN_QUALITY\"]].copy()\n",
                "\n",
                "# Standardize data for KMeans\n",
                "cluster_data_scaled = (cluster_data - cluster_data.mean()) / cluster_data.std()\n",
                "# print(cluster_data.head())\n",
                "# print(cluster_data_scaled.head())\n",
                "\n",
                "# Determine optimal number of clusters\n",
                "inertia = []\n",
                "k_range = range(1, 10)\n",
                "for k in k_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
                "    kmeans.fit(cluster_data_scaled)\n",
                "    inertia.append(kmeans.inertia_)\n",
                "\n",
                "# Plot elbow method\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.plot(k_range, inertia, marker=\"o\")\n",
                "plt.title(\"Elbow Method\")\n",
                "plt.xlabel(\"Number of Clusters\")\n",
                "plt.ylabel(\"Inertia\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "\n",
                "# Choose optimal k\n",
                "optimal_k = 3\n",
                "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
                "df_clients[\"CLUSTER\"] = kmeans.fit_predict(cluster_data_scaled)\n",
                "\n",
                "# Analyze clusters\n",
                "cluster_stats = df_clients.groupby(\"CLUSTER\").agg({\n",
                "    \"SELLING_HOURLY_PRICE\": [\"mean\", \"min\", \"max\", \"std\", \"count\"],\n",
                "    \"MIN_QUALITY\": [\"mean\", \"min\", \"max\", \"std\"],\n",
                "    \"PRICE_PER_QUALITY\": [\"mean\", \"min\", \"max\", \"std\"],\n",
                "    \"WILDCARD\": lambda x: x.value_counts().index[0]  # Most common WILDCARD\n",
                "}).round(2)\n",
                "\n",
                "print(\"Client cluster statistics:\")\n",
                "display(cluster_stats.T)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Cluster distribution:\")\n",
                "cluster_counts = df_clients[\"CLUSTER\"].value_counts().sort_index()\n",
                "cluster_percentages = cluster_counts / len(df_clients) * 100\n",
                "\n",
                "for cluster, count in cluster_counts.items():\n",
                "    percentage = cluster_percentages[cluster]\n",
                "    print(f\"  Cluster {cluster}: {count} clients ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize clusters\n",
                "plt.figure(figsize=(14, 8))\n",
                "scatter = plt.scatter(\n",
                "    df_clients[\"SELLING_HOURLY_PRICE\"], \n",
                "    df_clients[\"MIN_QUALITY\"],\n",
                "    c=df_clients[\"CLUSTER\"], \n",
                "    cmap=\"YlGnBu\",\n",
                "    s=100, \n",
                "    alpha=0.75\n",
                ")\n",
                "plt.colorbar(scatter, label=\"Cluster\")\n",
                "plt.title(\"Client Clusters by Price and Quality\")\n",
                "plt.xlabel(\"Selling Hourly Price\")\n",
                "plt.ylabel(\"Minimum Quality Requirement\")\n",
                "\n",
                "# Add cluster centroids\n",
                "centroids = kmeans.cluster_centers_\n",
                "centroids_unscaled = centroids * cluster_data.std().values + cluster_data.mean().values\n",
                "plt.scatter(\n",
                "    centroids_unscaled[:, 0], \n",
                "    centroids_unscaled[:, 1], \n",
                "    s=50, \n",
                "    marker=\"x\", \n",
                "    c=\"red\", \n",
                "    label=\"Centroids\"\n",
                ")\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify potential client segments based on price sensitivity\n",
                "price_quality_ratio = df_clients[\"SELLING_HOURLY_PRICE\"] / df_clients[\"MIN_QUALITY\"]\n",
                "\n",
                "df_clients[\"PRICE_SENSITIVITY\"] = pd.qcut(\n",
                "    price_quality_ratio, \n",
                "    q=3, \n",
                "    labels=[\"High\", \"Medium\", \"Low\"]\n",
                ")\n",
                "\n",
                "print(\"Client segments by price sensitivity:\")\n",
                "price_sens_counts = df_clients[\"PRICE_SENSITIVITY\"].value_counts().sort_index()\n",
                "price_sens_percentages = price_sens_counts / len(df_clients) * 100\n",
                "\n",
                "for sensitivity, count in price_sens_counts.items():\n",
                "    percentage = price_sens_percentages[sensitivity]\n",
                "    print(f\"- {sensitivity} price sensitivity: {count} clients ({percentage:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-tabulate price sensitivity vs WILDCARD\n",
                "price_sens_wildcard_crosstab = pd.crosstab(\n",
                "    df_clients[\"PRICE_SENSITIVITY\"], \n",
                "    df_clients[\"WILDCARD\"],\n",
                "    normalize=\"index\"\n",
                ") * 100\n",
                "\n",
                "print(\"Relationship between price sensitivity and WILDCARD (%):\")\n",
                "display(price_sens_wildcard_crosstab)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"KEY FINDINGS FROM CLIENT ANALYSIS:\")\n",
                "print()\n",
                "print(\"Total unique clients:\")\n",
                "print(f\"    {df_clients['CLIENT_NAME'].nunique()}\")\n",
                "print()\n",
                "print(\"Average selling price and price range:\")\n",
                "print(f\"    {df_clients['SELLING_HOURLY_PRICE'].mean():.2f}€ ({df_clients['SELLING_HOURLY_PRICE'].min():.2f}€ to {df_clients['SELLING_HOURLY_PRICE'].max():.2f}€)\")\n",
                "print()\n",
                "print(\"Average minimum quality requirement:\")\n",
                "print(f\"    {df_clients['MIN_QUALITY'].mean():.2f}/10\")\n",
                "print()\n",
                "print(\"Distinct client clusters:\")\n",
                "print(\"    \" + str(optimal_k))\n",
                "print()\n",
                "print(\"Client price sensitivity counts:\")\n",
                "print(f\"    High: {price_sens_counts['High']}, Medium: {price_sens_counts['Medium']}, Low: {price_sens_counts['Low']}\")\n",
                "print()\n",
                "print(\"Most common WILDCARD value:\")\n",
                "print(f\"    '{df_clients['WILDCARD'].mode()[0]}' ({wildcard_percentage[df_clients['WILDCARD'].mode()[0]]:.2f}% of clients)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### CSV Schedules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Basic information:\")\n",
                "df_schedules.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Statistical summary:\")\n",
                "\n",
                "numeric_cols = df_schedules.select_dtypes(include=[\"number\"])  # Select only numeric columns\n",
                "schedules_stats = numeric_cols.describe()\n",
                "schedules_stats.loc[\"skew\"] = numeric_cols.skew()\n",
                "schedules_stats.loc[\"kurtosis\"] = numeric_cols.kurtosis()\n",
                "display(schedules_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"CSV Schedules Data\")\n",
                "display(df_schedules.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "allday_cols = [\"MON\", \"TUES\", \"WED\", \"THURS\", \"FRI\", \"SAT\", \"SUN\"]  # All days of the week\n",
                "weekday_cols = [\"MON\", \"TUES\", \"WED\", \"THURS\", \"FRI\"]               # Weekdays only\n",
                "weekend_cols = [\"SAT\", \"SUN\"]                                       # Weekend days only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply the time_difference() function (assuming START and END are strings)\n",
                "df_schedules[\"HOURS\"] = df_schedules.apply(\n",
                "    lambda row: time_difference(row[\"START\"], row[\"END\"]),\n",
                "    axis=1\n",
                ")\n",
                "\n",
                "total_hours_per_name = df_schedules.groupby(\"NAME\")[\"HOURS\"].sum().reset_index()  # Total available hours per translator\n",
                "\n",
                "# Display results with better formatting and explicit sorting\n",
                "print(\"Total Hours Available per Name:\")\n",
                "display(total_hours_per_name.sort_values(by=\"HOURS\", ascending=True))  # Sort by hours ascending"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate weekday and weekend hours directly\n",
                "weekday_hours = df_schedules[weekday_cols].sum(axis=1)\n",
                "weekend_hours = df_schedules[weekend_cols].sum(axis=1)\n",
                "\n",
                "# Create a heatmap of weekday vs weekend hours\n",
                "heatmap_data, x_edges, y_edges = np.histogram2d(weekday_hours, weekend_hours, bins=(6, 3))\n",
                "\n",
                "# Plot the heatmap\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.imshow(heatmap_data.T, origin=\"lower\", cmap=\"YlGnBu\", aspect=\"auto\",\n",
                "           extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]])\n",
                "plt.colorbar(label=\"Frequency\")\n",
                "plt.title(\"Heatmap of Weekday vs Weekend Availability\")\n",
                "plt.xlabel(\"Weekday Hours\")\n",
                "plt.ylabel(\"Weekend Hours\")\n",
                "plt.xticks(range(int(x_edges[0]), int(x_edges[-1]) + 1))\n",
                "plt.yticks(range(int(y_edges[0]), int(y_edges[-1]) + 1))\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate maximum availability hours for each translator\n",
                "df_schedules[\"DAILY_HOURS\"] = df_schedules.apply(\n",
                "    lambda row: time_difference(row[\"START\"], row[\"END\"]),\n",
                "    axis=1\n",
                ")\n",
                "df_schedules[\"ACTIVE_DAYS\"] = df_schedules[allday_cols].sum(axis=1) # Count active days for each translator\n",
                "df_schedules[\"WEEKLY_HOURS\"] = df_schedules[\"DAILY_HOURS\"] * df_schedules[\"ACTIVE_DAYS\"] # Calculate total weekly hours\n",
                "\n",
                "# Weekly hours distribution\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_schedules[\"WEEKLY_HOURS\"], bins=30, kde=True, alpha=0.75)\n",
                "plt.title(\"Distribution of Weekly Availability Hours\")\n",
                "plt.xlabel(\"Weekly Hours\")\n",
                "plt.ylabel(\"Count Workers\")\n",
                "plt.axvline(df_schedules[\"WEEKLY_HOURS\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {df_schedules['WEEKLY_HOURS'].mean():.2f}\")\n",
                "plt.axvline(df_schedules[\"WEEKLY_HOURS\"].median(), color=\"blue\", linestyle=\"--\", label=f\"Median: {df_schedules['WEEKLY_HOURS'].median():.2f}\")\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Weekly Availability Hours Statistics:\")\n",
                "print(df_schedules[\"WEEKLY_HOURS\"].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate daily averages for each translator (excluding zeros)\n",
                "df_schedules[\"AVG_DAILY_HOURS\"] = df_schedules[allday_cols].replace(0, np.nan).mean(axis=1, skipna=True)\n",
                "df_schedules[\"WORKING_DAYS\"] = (df_schedules[allday_cols] > 0).sum(axis=1) # Calculate the number of working days availability per week\n",
                "df_schedules[\"HOURS_CONSISTENCY\"] = df_schedules[allday_cols].replace(0, np.nan).std(axis=1, skipna=True)\n",
                "\n",
                "# Identify weekend workers\n",
                "df_schedules[\"WEEKEND_WORKER\"] = (df_schedules[[\"SAT\", \"SUN\"]].sum(axis=1) > 0).astype(int)\n",
                "\n",
                "conditions = [\n",
                "    (df_schedules[\"WORKING_DAYS\"] >= 5),\n",
                "    (df_schedules[\"WORKING_DAYS\"] < 5) & (df_schedules[\"WORKING_DAYS\"] >= 3),\n",
                "    (df_schedules[\"WORKING_DAYS\"] < 3)\n",
                "]\n",
                "pattern_values = [\"Full-Time\", \"Part-Time\", \"Occasional\"]\n",
                "df_schedules[\"SCHEDULE_PATTERN\"] = np.select(conditions, pattern_values, default=\"Other\")\n",
                "\n",
                "# Display statistics about schedule patterns\n",
                "pattern_counts = df_schedules[\"SCHEDULE_PATTERN\"].value_counts()\n",
                "pattern_percentage = pattern_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"Schedule Pattern Distribution:\")\n",
                "for pattern, count in pattern_counts.items():\n",
                "    percentage = pattern_percentage[pattern]\n",
                "    print(f\"  {pattern}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize schedule patterns\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"SCHEDULE_PATTERN\", data=df_schedules, \n",
                "                  order=pattern_values, alpha=0.75)\n",
                "plt.title(\"Distribution of Schedule Patterns\")\n",
                "plt.xlabel(\"Count\")\n",
                "plt.ylabel(\"Schedule Pattern\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_schedules)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "\n",
                "for pattern, group in df_schedules.groupby(\"SCHEDULE_PATTERN\"):\n",
                "    print(f\"\\nGroup: {pattern}\")\n",
                "    print(f\"Number of translators: {len(group)}\")\n",
                "    display(group[allday_cols + [\"WORKING_DAYS\"]].head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the percentage of translators working each day\n",
                "day_percentage = (df_schedules[allday_cols] > 0).mean() * 100\n",
                "\n",
                "# Plot percentage of translators working each day\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.barplot(x=day_percentage.index, y=day_percentage.values, alpha=0.75)\n",
                "plt.title(\"Percentage of Translators Working by Day of Week\")\n",
                "plt.xlabel(\"Day of Week\")\n",
                "plt.ylabel(\"Percentage Workers\")\n",
                "\n",
                "# Add percentage labels on top of bars\n",
                "for i, v in enumerate(day_percentage):\n",
                "    ax.text(i, v + 1, f\"{v:.1f}%\", ha=\"center\")\n",
                "\n",
                "# Format y-axis ticks to include percentage signs\n",
                "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract hours from START and END times\n",
                "df_schedules[\"START_HOUR\"] = df_schedules[\"START\"].str.split(\":\", expand=True)[0].astype(int)\n",
                "df_schedules[\"END_HOUR\"] = df_schedules[\"END\"].str.split(\":\", expand=True)[0].astype(int)\n",
                "\n",
                "df_schedules[\"OVERNIGHT\"] = (df_schedules[\"END_HOUR\"] < df_schedules[\"START_HOUR\"]).astype(int) # Account for overnight shifts where END_HOUR < START_HOUR\n",
                "\n",
                "df_schedules[\"END_HOUR_ADJ\"] = np.where( # Adjust END_HOUR for visualization (add 24 to overnight shift end hours)\n",
                "    df_schedules[\"OVERNIGHT\"] == 1,\n",
                "    df_schedules[\"END_HOUR\"] + 24,\n",
                "    df_schedules[\"END_HOUR\"]\n",
                ")\n",
                "\n",
                "# Define work shift categories\n",
                "conditions = [\n",
                "    (df_schedules[\"START_HOUR\"] >= 5) & (df_schedules[\"START_HOUR\"] < 9),  # Morning shift\n",
                "    (df_schedules[\"START_HOUR\"] >= 9) & (df_schedules[\"START_HOUR\"] < 12),  # Mid-morning shift\n",
                "    (df_schedules[\"START_HOUR\"] >= 12) & (df_schedules[\"START_HOUR\"] < 17),  # Afternoon shift\n",
                "    (df_schedules[\"START_HOUR\"] >= 17) & (df_schedules[\"START_HOUR\"] < 22),  # Evening shift\n",
                "    (df_schedules[\"START_HOUR\"] >= 22) | (df_schedules[\"START_HOUR\"] < 5)   # Night shift\n",
                "]\n",
                "shift_values = [\"Morning\", \"Mid-Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n",
                "df_schedules[\"WORK_SHIFT\"] = np.select(conditions, shift_values, default=\"Other\")\n",
                "\n",
                "# Display shift statistics\n",
                "shift_counts = df_schedules[\"WORK_SHIFT\"].value_counts()\n",
                "shift_percentage = shift_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"Work Shift Distribution:\")\n",
                "for shift, count in shift_counts.items():\n",
                "    percentage = shift_percentage[shift]\n",
                "    print(f\"  {shift}: {count} schedules ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize shift distribution\n",
                "plt.figure(figsize=(14, 8))\n",
                "order = [\"Morning\", \"Mid-Morning\", \"Afternoon\", \"Evening\", \"Night\", \"Other\"]\n",
                "order = [shift for shift in order if shift in shift_counts.index]  # Only include shifts that exist\n",
                "ax = sns.countplot(y=\"WORK_SHIFT\", data=df_schedules, order=order, alpha=0.75)\n",
                "plt.title(\"Distribution of Work Shifts\")\n",
                "plt.xlabel(\"Count\")\n",
                "plt.ylabel(\"Work Shift\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_schedules)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Visualize start and end hour distribution\n",
                "plt.figure(figsize=(16, 10))\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.histplot(df_schedules[\"START_HOUR\"], bins=24, kde=True, alpha=0.75)\n",
                "plt.title(\"Distribution of Start Hours\")\n",
                "plt.xlabel(\"Hour of Day (24-hour format)\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.xticks(range(0, 24))\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "sns.histplot(df_schedules[\"END_HOUR\"], bins=24, kde=True, alpha=0.75)\n",
                "plt.title(\"Distribution of End Hours\")\n",
                "plt.xlabel(\"Hour of Day (24-hour format)\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.xticks(range(0, 24))\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a matrix of availability by hour and day\n",
                "hours = range(24)\n",
                "availability_matrix = np.zeros((24, 7))\n",
                "\n",
                "for idx, row in df_schedules.iterrows():\n",
                "    start_hour = row[\"START_HOUR\"]\n",
                "    end_hour = row[\"END_HOUR_ADJ\"] if row[\"OVERNIGHT\"] == 1 else row[\"END_HOUR\"]\n",
                "    \n",
                "    # For each day of the week\n",
                "    for day_idx, day in enumerate(allday_cols):\n",
                "        if row[day] > 0: # If the translator works on this day\n",
                "            for hour in range(start_hour, end_hour): # Mark all hours from start to end as available\n",
                "                hour_idx = hour % 24  # Wrap around for overnight shifts\n",
                "                availability_matrix[hour_idx, day_idx] += 1\n",
                "\n",
                "# Normalize by the total number of translators\n",
                "availability_percentage = (availability_matrix / len(df_schedules)) * 100\n",
                "\n",
                "# Visualize the availability heatmap\n",
                "plt.figure(figsize=(18, 10))\n",
                "sns.heatmap(availability_percentage, annot=False, cmap=\"YlGnBu\", \n",
                "            xticklabels=allday_cols, yticklabels=hours, fmt=\".1f\")\n",
                "plt.title(\"Translator Availability by Hour and Day (%)\")\n",
                "plt.xlabel(\"Day of Week\")\n",
                "plt.ylabel(\"Hour of Day\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Identify peak and low availability times\n",
                "hour_availability = availability_percentage.mean(axis=1)  # Average across days\n",
                "day_availability = availability_percentage.mean(axis=0)   # Average across hours\n",
                "\n",
                "# Display peak and low availability times\n",
                "peak_hour = hours[np.argmax(hour_availability)]\n",
                "low_hour = hours[np.argmin(hour_availability)]\n",
                "peak_day_idx = np.argmax(day_availability)\n",
                "low_day_idx = np.argmin(day_availability)\n",
                "\n",
                "print(\"Peak and Low Availability:\")\n",
                "print(f\"  Peak hour: {peak_hour}:00 with {hour_availability[peak_hour]:.2f}% of translators available\")\n",
                "print(f\"  Lowest availability hour: {low_hour}:00 with {hour_availability[low_hour]:.2f}% of translators available\")\n",
                "print(f\"  Peak day: {allday_cols[peak_day_idx]} with {day_availability[peak_day_idx]:.2f}% of translators available\")\n",
                "print(f\"  Lowest availability day: {allday_cols[low_day_idx]} with {day_availability[low_day_idx]:.2f}% of translators available\")\n",
                "\n",
                "# Visualize hourly and daily availability\n",
                "plt.figure(figsize=(18, 8))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.barplot(x=list(hours), y=hour_availability, alpha=0.75)\n",
                "plt.title(\"Average Translator Availability by Hour\")\n",
                "plt.xlabel(\"Hour of Day\")\n",
                "plt.ylabel(\"Percentage Available (%)\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "sns.barplot(x=allday_cols, y=day_availability, alpha=0.75)\n",
                "plt.title(\"Average Translator Availability by Day\")\n",
                "plt.xlabel(\"Day of Week\")\n",
                "plt.ylabel(\"Percentage Available (%)\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define work pattern based on hours\n",
                "df_schedules[\"HOURS_WORKED\"] = df_schedules.apply(\n",
                "    lambda row: time_difference(row[\"START\"], row[\"END\"]),\n",
                "    axis=1\n",
                ")\n",
                "\n",
                "df_schedules[\"WORK_PATTERN\"] = pd.cut(\n",
                "    df_schedules[\"HOURS_WORKED\"],\n",
                "    bins=[0, 4, 6, 8, 12, 24],\n",
                "    labels=[\"Part-time (<4h)\", \"Part-time (4-6h)\", \"Standard (6-8h)\", \"Extended (8-12h)\", \"Long (>12h)\"]\n",
                ")\n",
                "\n",
                "work_pattern_counts = df_schedules[\"WORK_PATTERN\"].value_counts().sort_index()\n",
                "work_pattern_percentage = work_pattern_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"\\nWork Pattern Distribution:\")\n",
                "for pattern, count in work_pattern_counts.items():\n",
                "    percentage = work_pattern_percentage[pattern]\n",
                "    print(f\"- {pattern}: {count} records ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize work patterns\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"WORK_PATTERN\", data=df_schedules, order=work_pattern_counts.index, alpha=0.75)\n",
                "\n",
                "# Add percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_schedules)\n",
                "    ax.text(width + 1, p.get_y() + p.get_height()/2, f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.title(\"Distribution of Working Hour Patterns\")\n",
                "plt.xlabel(\"Count of Translators\")\n",
                "plt.ylabel(\"Work Pattern\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze schedule flexibility across weekdays\n",
                "df_schedules[\"WEEKDAY_CONSISTENCY\"] = df_schedules[weekday_cols].std(axis=1) / df_schedules[weekday_cols].mean(axis=1)\n",
                "# df_schedules[\"WEEKDAY_CONSISTENCY\"].fillna(0, inplace=True)  # Replace NaN (from division by zero) with 0\n",
                "\n",
                "# Define flexibility categories\n",
                "df_schedules[\"SCHEDULE_FLEXIBILITY\"] = pd.cut(\n",
                "    df_schedules[\"WEEKDAY_CONSISTENCY\"],\n",
                "    bins=[-0.001, 0.1, 0.3, 0.6, float('inf')], # Start at -0.001 to include 0\n",
                "    labels=[\"Very Consistent\", \"Consistent\", \"Moderately Flexible\", \"Highly Flexible\"]\n",
                ")\n",
                "\n",
                "# Calculate statistics\n",
                "flexibility_counts = df_schedules[\"SCHEDULE_FLEXIBILITY\"].value_counts().sort_index()\n",
                "flexibility_percentage = flexibility_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"Schedule Flexibility Distribution:\")\n",
                "for flexibility, count in flexibility_counts.items():\n",
                "    percentage = flexibility_percentage[flexibility]\n",
                "    print(f\"  {flexibility}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize schedule flexibility\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"SCHEDULE_FLEXIBILITY\", data=df_schedules, \n",
                "                   order=flexibility_counts.index, alpha=0.75)\n",
                "plt.title(\"Distribution of Schedule Flexibility Across Weekdays\")\n",
                "plt.xlabel(\"Count of Translators\")\n",
                "plt.ylabel(\"Schedule Flexibility\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_schedules)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Cross-tabulate schedule flexibility with work pattern\n",
                "flexibility_vs_pattern = pd.crosstab(\n",
                "    df_schedules[\"SCHEDULE_FLEXIBILITY\"], \n",
                "    df_schedules[\"WORK_PATTERN\"],\n",
                "    normalize=\"index\"\n",
                ") * 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if \"TOTAL_HOURS_PER_WEEK\" not in df_schedules.columns:\n",
                "    df_schedules[\"TOTAL_HOURS_PER_WEEK\"] = df_schedules[\"DAILY_HOURS\"] * df_schedules[\"ACTIVE_DAYS\"]\n",
                "\n",
                "\n",
                "# Identify unusual schedules for potential workload optimization\n",
                "\n",
                "# -- Very long shifts (more than 12 hours)\n",
                "long_shifts = df_schedules[df_schedules[\"HOURS_WORKED\"] > 12]\n",
                "print(f\"Translators with very long shifts (>12 hours): {len(long_shifts)} ({len(long_shifts)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Very short shifts (less than 3 hours)\n",
                "short_shifts = df_schedules[df_schedules[\"HOURS_WORKED\"] < 3]\n",
                "print(f\"Translators with very short shifts (<3 hours): {len(short_shifts)} ({len(short_shifts)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Overnight shifts (crossing midnight)\n",
                "overnight_shifts = df_schedules[df_schedules[\"OVERNIGHT\"] == 1]\n",
                "print(f\"Translators with overnight shifts: {len(overnight_shifts)} ({len(overnight_shifts)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Weekend-only workers\n",
                "weekend_only = df_schedules[\n",
                "    (df_schedules[[\"MON\", \"TUES\", \"WED\", \"THURS\", \"FRI\"]].sum(axis=1) == 0) &\n",
                "    (df_schedules[[\"SAT\", \"SUN\"]].sum(axis=1) > 0)\n",
                "]\n",
                "print(f\"\\nWeekend-only workers: {len(weekend_only)} ({len(weekend_only)/len(df_schedules)*100:.2f}%)\\n\")\n",
                "\n",
                "# -- Very high weekly hours (more than 60 hours)\n",
                "high_weekly = df_schedules[df_schedules[\"TOTAL_HOURS_PER_WEEK\"] > 60]\n",
                "print(f\"Translators with high weekly hours (>60): {len(high_weekly)} ({len(high_weekly)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Very low weekly hours (less than 10 hours)\n",
                "low_weekly = df_schedules[df_schedules[\"TOTAL_HOURS_PER_WEEK\"] < 10]\n",
                "print(f\"Translators with low weekly hours (<10): {len(low_weekly)} ({len(low_weekly)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Single day workers\n",
                "single_day = df_schedules[df_schedules[\"WORKING_DAYS\"] == 1]\n",
                "print(f\"Single day workers: {len(single_day)} ({len(single_day)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "# -- Unusual start times (very early or late)\n",
                "unusual_start = df_schedules[\n",
                "    (df_schedules[\"START_HOUR\"] < 5) | \n",
                "    ((df_schedules[\"START_HOUR\"] > 11) & (df_schedules[\"START_HOUR\"] < 14)) |\n",
                "    (df_schedules[\"START_HOUR\"] > 19)\n",
                "]\n",
                "print(f\"Translators with unusual start times: {len(unusual_start)} ({len(unusual_start)/len(df_schedules)*100:.2f}%)\")\n",
                "\n",
                "all_unusual = pd.concat([ # Combine all unusual schedules and count unique translators\n",
                "    long_shifts, short_shifts, overnight_shifts, weekend_only,\n",
                "    high_weekly, low_weekly, single_day, unusual_start\n",
                "])\n",
                "unique_unusual = all_unusual[\"NAME\"].nunique()\n",
                "print(f\"\\nTotal unique translators with unusual schedules: {unique_unusual} ({unique_unusual/df_schedules['NAME'].nunique()*100:.2f}% of all translators)\")\n",
                "\n",
                "# Visualize the distribution of unusual schedule types\n",
                "unusual_counts = {\n",
                "    \"Long Shifts\": len(long_shifts),\n",
                "    \"Short Shifts\": len(short_shifts),\n",
                "    \"Overnight Shifts\": len(overnight_shifts),\n",
                "    \"Weekend Only\": len(weekend_only),\n",
                "    \"High Weekly Hours\": len(high_weekly),\n",
                "    \"Low Weekly Hours\": len(low_weekly),\n",
                "    \"Single Day\": len(single_day),\n",
                "    \"Unusual Start Time\": len(unusual_start)\n",
                "}\n",
                "\n",
                "# Sort by frequency\n",
                "unusual_counts = {k: v for k, v in sorted(unusual_counts.items(), key=lambda item: item[1], reverse=True)}\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.bar(unusual_counts.keys(), unusual_counts.values(), alpha=0.75)\n",
                "plt.title(\"Distribution of Unusual Schedule Types\")\n",
                "plt.xlabel(\"Type of Unusual Schedule\")\n",
                "plt.ylabel(\"Number of Translators\")\n",
                "plt.xticks(rotation=45, ha=\"right\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate utilization metrics\n",
                "max_hours_per_week = 7 * 24\n",
                "\n",
                "# Actual hours worked as a percentage of total possible hours\n",
                "df_schedules[\"UTILIZATION_RATE\"] = (df_schedules[\"TOTAL_HOURS_PER_WEEK\"] / max_hours_per_week) * 100\n",
                "\n",
                "# Calculate organizational utilization metrics\n",
                "avg_utilization = df_schedules[\"UTILIZATION_RATE\"].mean()\n",
                "median_utilization = df_schedules[\"UTILIZATION_RATE\"].median()\n",
                "max_utilization = df_schedules[\"UTILIZATION_RATE\"].max()\n",
                "min_utilization = df_schedules[\"UTILIZATION_RATE\"].min()\n",
                "\n",
                "print(\"Translator Utilization Metrics:\")\n",
                "print(f\"Average utilization rate: {avg_utilization:.2f}%\")\n",
                "print(f\"Median utilization rate: {median_utilization:.2f}%\")\n",
                "print(f\"Maximum utilization rate: {max_utilization:.2f}%\")\n",
                "print(f\"Minimum utilization rate: {min_utilization:.2f}%\")\n",
                "\n",
                "# Define utilization categories\n",
                "df_schedules[\"UTILIZATION_CATEGORY\"] = pd.cut(\n",
                "    df_schedules[\"UTILIZATION_RATE\"],\n",
                "    bins=[0, 10, 20, 30, 40, 50, 100],\n",
                "    labels=[\"Very Low (<10%)\", \"Low (10-20%)\", \"Moderate (20-30%)\", \n",
                "            \"High (30-40%)\", \"Very High (40-50%)\", \"Extreme (>50%)\"]\n",
                ")\n",
                "\n",
                "# Display utilization category distribution\n",
                "util_category_counts = df_schedules[\"UTILIZATION_CATEGORY\"].value_counts().sort_index()\n",
                "util_category_percentage = util_category_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"\\nUtilization Category Distribution:\")\n",
                "for category, count in util_category_counts.items():\n",
                "    percentage = util_category_percentage[category]\n",
                "    print(f\"  {category}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize utilization categories\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"UTILIZATION_CATEGORY\", data=df_schedules, \n",
                "                   order=util_category_counts.index, alpha=0.75)\n",
                "plt.title(\"Distribution of Translator Utilization Categories\")\n",
                "plt.xlabel(\"Count of Translators\")\n",
                "plt.ylabel(\"Utilization Category\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_schedules)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate aggregate utilization metrics\n",
                "total_available_translator_hours = len(df_schedules) * max_hours_per_week\n",
                "total_scheduled_hours = df_schedules[\"TOTAL_HOURS_PER_WEEK\"].sum()\n",
                "organization_utilization = (total_scheduled_hours / total_available_translator_hours) * 100\n",
                "\n",
                "print(f\"\\nOrganization-wide Utilization:\")\n",
                "print(f\"Total available translator hours per week: {total_available_translator_hours:,.0f} hours\")\n",
                "print(f\"Total scheduled hours per week: {total_scheduled_hours:,.0f} hours\")\n",
                "print(f\"Organization-wide utilization rate: {organization_utilization:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define timezone categories based on shift start times\n",
                "conditions = [\n",
                "    (df_schedules[\"START_HOUR\"] >= 22) | (df_schedules[\"START_HOUR\"] < 6),  # Late night/early morning - Asia/Pacific\n",
                "    (df_schedules[\"START_HOUR\"] >= 6) & (df_schedules[\"START_HOUR\"] < 14),   # Morning to afternoon - Americas\n",
                "    (df_schedules[\"START_HOUR\"] >= 14) & (df_schedules[\"START_HOUR\"] < 22)   # Evening - Europe/Africa\n",
                "]\n",
                "timezone_values = [\"Asia/Pacific\", \"Americas\", \"Europe/Africa\"]\n",
                "df_schedules[\"LIKELY_TIMEZONE\"] = np.select(conditions, timezone_values, default=\"Unknown\")\n",
                "\n",
                "# Calculate timezone distribution\n",
                "timezone_counts = df_schedules[\"LIKELY_TIMEZONE\"].value_counts()\n",
                "timezone_percentage = timezone_counts / len(df_schedules) * 100\n",
                "\n",
                "print(\"Likely Timezone Coverage:\")\n",
                "for timezone, count in timezone_counts.items():\n",
                "    percentage = timezone_percentage[timezone]\n",
                "    print(f\"  {timezone}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize timezone distribution\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.barplot(x=timezone_counts.index, y=timezone_counts.values, alpha=0.75)\n",
                "plt.title(\"Distribution of Translators by Likely Timezone Region\")\n",
                "plt.xlabel(\"Timezone Region\")\n",
                "plt.ylabel(\"Number of Translators\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    height = p.get_height()\n",
                "    percentage = 100 * height / len(df_schedules)\n",
                "    ax.text(p.get_x() + p.get_width()/2, height + 5, \n",
                "            f\"{height} ({percentage:.1f}%)\", ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Analyze 24/7 coverage\n",
                "coverage_by_hour = [availability_matrix[hour, :].sum() > 0 for hour in range(24)]\n",
                "all_hours_covered = all(coverage_by_hour)\n",
                "\n",
                "print(f\"\\n24/7 Coverage Analysis:\")\n",
                "if all_hours_covered:\n",
                "    print(\"All 24 hours have at least one translator available on at least one day of the week.\")\n",
                "else:\n",
                "    uncovered_hours = [hour for hour, covered in enumerate(coverage_by_hour) if not covered]\n",
                "    print(f\"The following hours have no translator coverage on any day: {uncovered_hours}\")\n",
                "\n",
                "# Check if all days have coverage for all hours\n",
                "full_day_coverage = []\n",
                "for day_idx, day in enumerate(allday_cols):\n",
                "    day_coverage = [availability_matrix[hour, day_idx] > 0 for hour in range(24)]\n",
                "    if all(day_coverage):\n",
                "        full_day_coverage.append(day)\n",
                "\n",
                "if full_day_coverage:\n",
                "    print(f\"The following days have 24-hour coverage: {', '.join(full_day_coverage)}\")\n",
                "else:\n",
                "    print(\"No day has complete 24-hour coverage.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"KEY FINDINGS FROM SCHEDULE ANALYSIS:\")\n",
                "print()\n",
                "print(\"Workforce Size and Structure:\")\n",
                "print(f\"    {df_schedules['NAME'].nunique()} unique translators with varying schedule patterns\")\n",
                "print()\n",
                "print(\"Working Hours:\")\n",
                "print(f\"    Average: {df_schedules['TOTAL_HOURS_PER_WEEK'].mean():.2f} hours per week\")\n",
                "print(f\"    Median: {df_schedules['TOTAL_HOURS_PER_WEEK'].median():.2f} hours per week\")\n",
                "print()\n",
                "print(\"Shift Duration:\")\n",
                "print(f\"    Average daily shift: {df_schedules['HOURS_WORKED'].mean():.2f} hours\")\n",
                "print(f\"    Standard shifts (6-8h): {len(df_schedules[df_schedules['WORK_PATTERN'] == 'Standard (6-8h)'])} translators ({len(df_schedules[df_schedules['WORK_PATTERN'] == 'Standard (6-8h)'])/len(df_schedules)*100:.2f}%)\")\n",
                "print()\n",
                "print(\"Schedule Patterns:\")\n",
                "print(f\"    Distinct schedule clusters: {optimal_k}\")\n",
                "print(f\"    Largest cluster: {cluster_counts.max()} translators ({cluster_percentages.max():.2f}% of workforce)\")\n",
                "print()\n",
                "print(\"Workforce Flexibility:\")\n",
                "print(f\"    Highly flexible schedules: {len(df_schedules[df_schedules['SCHEDULE_FLEXIBILITY'] == 'Highly Flexible'])} translators ({len(df_schedules[df_schedules['SCHEDULE_FLEXIBILITY'] == 'Highly Flexible'])/len(df_schedules)*100:.2f}%)\")\n",
                "print()\n",
                "print(\"Weekend Coverage:\")\n",
                "print(f\"    Weekend workers: {len(df_schedules[df_schedules['WEEKEND_WORKER'] == 1])} translators ({len(df_schedules[df_schedules['WEEKEND_WORKER'] == 1])/len(df_schedules)*100:.2f}%)\")\n",
                "print(f\"    Saturday availability: {day_percentage['SAT']:.2f}%\")\n",
                "print(f\"    Sunday availability: {day_percentage['SUN']:.2f}%\")\n",
                "print()\n",
                "print(\"Timezone Coverage:\")\n",
                "print(f\"    Americas: {timezone_percentage['Americas']:.2f}%\")\n",
                "print(f\"    Europe/Africa: {timezone_percentage['Europe/Africa']:.2f}%\")\n",
                "print(f\"    Asia/Pacific: {timezone_percentage['Asia/Pacific']:.2f}%\")\n",
                "print()\n",
                "print(\"Peak Availability:\")\n",
                "print(f\"    Highest availability: {allday_cols[peak_day_idx]} at {peak_hour}:00 ({hour_availability[peak_hour]:.2f}% of translators)\")\n",
                "print()\n",
                "print(\"Utilization Rate:\")\n",
                "print(f\"    Average translator utilization: {avg_utilization:.2f}%\")\n",
                "print(f\"    Organization-wide utilization: {organization_utilization:.2f}%\")\n",
                "print()\n",
                "print(\"Unusual Schedules:\")\n",
                "print(f\"    Translators with unusual schedules: {unique_unusual} ({unique_unusual/df_schedules['NAME'].nunique()*100:.2f}%)\")\n",
                "print(f\"    Overnight shifts: {len(overnight_shifts)} translators\")\n",
                "print()\n",
                "print(\"Schedule Consistency:\")\n",
                "print(f\"    Very consistent weekday schedules: {len(df_schedules[df_schedules['SCHEDULE_FLEXIBILITY'] == 'Very Consistent'])} translators ({len(df_schedules[df_schedules['SCHEDULE_FLEXIBILITY'] == 'Very Consistent'])/len(df_schedules)*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### CSV Translators Cost Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Basic information:\")\n",
                "df_translators.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Number of unique translators: {df_translators['TRANSLATOR'].nunique()}\")\n",
                "print(f\"Number of source languages: {df_translators['SOURCE_LANG'].nunique()}\")\n",
                "print(f\"Number of target languages: {df_translators['TARGET_LANG'].nunique()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Number of unique language pairs: {df_translators.groupby(['SOURCE_LANG', 'TARGET_LANG']).ngroups}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of hourly rates\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.hist(df_translators[\"HOURLY_RATE\"], bins=50, alpha=0.75, edgecolor=\"black\")\n",
                "plt.title(\"Distribution of Hourly Rates\")\n",
                "plt.xlabel(\"Hourly Rate\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Obtain the language pairs\n",
                "pair_languages = df_translators.groupby([\"SOURCE_LANG\", \"TARGET_LANG\"]).size().reset_index(name=\"COUNT\")\n",
                "\n",
                "# Order by the most common language pairs\n",
                "pair_languages_top = pair_languages.sort_values(\"COUNT\", ascending=False).head(20)\n",
                "\n",
                "# Order by the less common language pairs\n",
                "pair_languages_bottom = pair_languages.sort_values(\"COUNT\", ascending=True).head(20)\n",
                "\n",
                "\n",
                "print(\"TOP Translator Pair Languages\")\n",
                "display(pair_languages_top)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "plt.barh(pair_languages_top.apply(lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']}\", axis=1), pair_languages_top[\"COUNT\"], alpha=0.75)\n",
                "plt.xlabel(\"Count Frequency\")\n",
                "plt.ylabel(\"Language Pair\")\n",
                "plt.title(\"Top Common Language Pairs\")\n",
                "plt.show()\n",
                "\n",
                "print(\"LEAST Common Translator Pair Languages\")\n",
                "display(pair_languages_bottom)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "plt.barh(pair_languages_bottom.apply(lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']}\", axis=1), pair_languages_bottom[\"COUNT\"], alpha=0.75)\n",
                "plt.xlabel(\"Count Frequency\")\n",
                "plt.ylabel(\"Language Pair\")\n",
                "plt.xticks(range(0, 11))\n",
                "plt.title(\"Top Uncommon Language Pairs\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Group data by translator and count language pairs\n",
                "translator_lang_pair_counts = df_translators.groupby(\"TRANSLATOR\").size().reset_index(name=\"language_pair_count\")\n",
                "\n",
                "# Print top translators\n",
                "print(\"Translators with most language pairs:\")\n",
                "print(translator_lang_pair_counts.sort_values(\"language_pair_count\", ascending=False).head(10))\n",
                "\n",
                "# Create the plot with Seaborn\n",
                "plt.figure(figsize=(12,6))\n",
                "sns.histplot(\n",
                "    data=translator_lang_pair_counts,\n",
                "    x=\"language_pair_count\",\n",
                "    bins=25,\n",
                "    stat=\"count\",          \n",
                "    alpha=0.75,\n",
                "    kde=True,              \n",
                ")\n",
                "\n",
                "# Customize the plot\n",
                "plt.title(\"Number of Language Pairs per Translator\")\n",
                "plt.xlabel(\"Number of Language Pairs\")\n",
                "plt.ylabel(\"Count of Translators\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation between rates and languages\n",
                "avg_rate_by_source = df_translators.groupby(\"SOURCE_LANG\")[\"HOURLY_RATE\"].mean().sort_values(ascending=False)\n",
                "avg_rate_by_target = df_translators.groupby(\"TARGET_LANG\")[\"HOURLY_RATE\"].mean().sort_values(ascending=False)\n",
                "\n",
                "print(\"Average hourly rate by source language:\")\n",
                "print(avg_rate_by_source.head(10))\n",
                "print()\n",
                "\n",
                "print(\"Average hourly rate by target language:\")\n",
                "print(avg_rate_by_target.head(10))\n",
                "\n",
                "# Visualize average rates for top source languages\n",
                "plt.figure(figsize=(14, 8))\n",
                "avg_rate_by_source.head(10).plot(kind=\"bar\", alpha=0.75)\n",
                "plt.title(\"Average Hourly Rate by Source Language\")\n",
                "plt.xlabel(\"Source Language\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Visualize average rates for top target languages\n",
                "plt.figure(figsize=(14, 8))\n",
                "avg_rate_by_target.head(10).plot(kind=\"bar\", alpha=0.75)\n",
                "plt.title(\"Average Hourly Rate by Target Language\")\n",
                "plt.xlabel(\"Target Language\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed analysis of hourly rates\n",
                "print(\"Detailed Hourly Rate Statistics:\")\n",
                "rate_stats = df_translators[\"HOURLY_RATE\"].describe()\n",
                "rate_stats[\"skew\"] = df_translators[\"HOURLY_RATE\"].skew()\n",
                "rate_stats[\"kurtosis\"] = df_translators[\"HOURLY_RATE\"].kurtosis()\n",
                "print(rate_stats)\n",
                "\n",
                "# Define rate bins and labels based on data distribution\n",
                "print(f\"\\nHourly Rate Range: Min = {df_translators['HOURLY_RATE'].min()}, Max = {df_translators['HOURLY_RATE'].max()}\")\n",
                "rate_bins = [0, 10, 15, 20, 30, 40, float('inf')]  # Adjusted to cover all possible rates\n",
                "rate_labels = [\n",
                "    \"Very Low (≤10)\",\n",
                "    \"Low (10-15)\",\n",
                "    \"Medium-Low (15-20)\",\n",
                "    \"Medium (20-30)\",\n",
                "    \"High (30-40)\",\n",
                "    \"Premium (>40)\"\n",
                "]\n",
                "\n",
                "# Create rate categories\n",
                "df_translators[\"RATE_CATEGORY\"] = pd.cut(\n",
                "    df_translators[\"HOURLY_RATE\"],\n",
                "    bins=rate_bins,\n",
                "    labels=rate_labels,\n",
                "    include_lowest=True\n",
                ")\n",
                "\n",
                "# Distribution of rate categories\n",
                "rate_cat_counts = df_translators[\"RATE_CATEGORY\"].value_counts().sort_index()\n",
                "rate_cat_percent = rate_cat_counts / len(df_translators) * 100\n",
                "\n",
                "print(\"\\nHourly Rate Category Distribution:\")\n",
                "for category, count in rate_cat_counts.items():\n",
                "    percentage = rate_cat_percent[category]\n",
                "    print(f\"  {category}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize rate categories\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"RATE_CATEGORY\", data=df_translators, \n",
                "                   order=rate_labels, alpha=0.75)  # Order by rate_labels for logical sequence\n",
                "plt.title(\"Distribution of Hourly Rate Categories\")\n",
                "plt.xlabel(\"Count\")\n",
                "plt.ylabel(\"Rate Category\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(df_translators)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width:.0f} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze average hourly rates for translators by language pair\n",
                "avg_rates_by_pair = df_translators.groupby([\"SOURCE_LANG\", \"TARGET_LANG\"])[\"HOURLY_RATE\"].agg([\n",
                "    \"mean\", \"median\", \"std\", \"min\", \"max\", \"count\"\n",
                "]).reset_index()\n",
                "\n",
                "# Sort by mean hourly rate (descending) and then by count (descending)\n",
                "avg_rates_by_pair = avg_rates_by_pair.sort_values([\"mean\", \"count\"], ascending=[False, False])\n",
                "\n",
                "# Display top highest-paid language pairs by average hourly rate\n",
                "print(\"Top Highest-Paid Language Pairs by Average Hourly Rate for Translators:\")\n",
                "display(avg_rates_by_pair.head(20))\n",
                "\n",
                "# Display top lowest-paid language pairs by average hourly rate (with at least 5 translators)\n",
                "print(\"\\nTop Lowest-Paid Language Pairs by Average Hourly Rate for Translators (min 5 translators):\")\n",
                "min_translators = 5\n",
                "display(avg_rates_by_pair[avg_rates_by_pair[\"count\"] >= min_translators].tail(20))\n",
                "\n",
                "# Visualize top highest-paid language pairs by average hourly rate\n",
                "top_pairs = avg_rates_by_pair.head(15)\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.barplot(\n",
                "    x=\"mean\", \n",
                "    y=top_pairs.apply(lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']} (n={x['count']})\", axis=1),\n",
                "    data=top_pairs,\n",
                "    alpha=0.75\n",
                ")\n",
                "plt.title(\"Top Highest-Paid Language Pairs by Average Hourly Rate for Translators\")\n",
                "plt.xlabel(\"Average Hourly Rate (€)\")\n",
                "plt.ylabel(\"Language Pair (with translator count)\")\n",
                "\n",
                "# Add rate labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    ax.text(width + 0.15, p.get_y() + p.get_height()/2, \n",
                "            f\"{width:.2f}€\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze rate variance within language pairs\n",
                "avg_rates_by_pair[\"cv\"] = avg_rates_by_pair[\"std\"] / avg_rates_by_pair[\"mean\"] # Calculate coefficient of variation (CV) for each language pair\n",
                "\n",
                "# Filter for language pairs with at least 5 translators\n",
                "min_translators = 5\n",
                "rate_variance = avg_rates_by_pair[avg_rates_by_pair[\"count\"] >= min_translators].copy()\n",
                "\n",
                "high_variance_pairs = rate_variance.sort_values(\"cv\", ascending=False).head(15) # Sort by coefficient of variation (descending)\n",
                "low_variance_pairs = rate_variance.sort_values(\"cv\", ascending=True).head(15) # Sort by coefficient of variation (ascending)\n",
                "\n",
                "# Display pairs with highest rate variance\n",
                "print(f\"Top Language Pairs with Highest Rate Variance (min {min_translators} translators):\")\n",
                "display(high_variance_pairs[[\"SOURCE_LANG\", \"TARGET_LANG\", \"mean\", \"std\", \"min\", \"max\", \"cv\", \"count\"]])\n",
                "\n",
                "# Display pairs with lowest rate variance\n",
                "print(f\"\\nTop Language Pairs with Lowest Rate Variance (min {min_translators} translators):\")\n",
                "display(low_variance_pairs[[\"SOURCE_LANG\", \"TARGET_LANG\", \"mean\", \"std\", \"min\", \"max\", \"cv\", \"count\"]])\n",
                "\n",
                "# Visualize rate range for high variance pairs\n",
                "plt.figure(figsize=(16, 10))\n",
                "high_variance_pairs_sorted = high_variance_pairs.sort_values(\"mean\", ascending=False) # Create a custom sorted index\n",
                "pair_labels = high_variance_pairs_sorted.apply(\n",
                "    lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']} (CV={x['cv']:.2f})\", axis=1\n",
                ")\n",
                "# Create error bars showing rate range\n",
                "plt.errorbar(\n",
                "    x=high_variance_pairs_sorted[\"mean\"],\n",
                "    y=range(len(high_variance_pairs_sorted)),\n",
                "    xerr=high_variance_pairs_sorted[\"std\"],\n",
                "    fmt=\"o\",\n",
                "    capsize=5,\n",
                "    alpha=0.7,\n",
                "    label=\"Mean ± Std Dev\"\n",
                ")\n",
                "\n",
                "# Add min-max range\n",
                "for i, (_, row) in enumerate(high_variance_pairs_sorted.iterrows()):\n",
                "    plt.plot([row[\"min\"], row[\"max\"]], [i, i], \"k-\", alpha=0.5, label=\"Min-Max Range\" if i == 0 else \"\")\n",
                "\n",
                "plt.yticks(range(len(pair_labels)), pair_labels)\n",
                "plt.title(\"Language Pairs with Highest Rate Variance\")\n",
                "plt.xlabel(\"Hourly Rate\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Visualize rate range for low variance pairs\n",
                "plt.figure(figsize=(16, 10))\n",
                "low_variance_pairs_sorted = low_variance_pairs.sort_values(\"mean\", ascending=False) # Create a custom sorted index\n",
                "pair_labels = low_variance_pairs_sorted.apply(\n",
                "    lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']} (CV={x['cv']:.2f})\", axis=1\n",
                ")\n",
                "# Create error bars showing rate range\n",
                "plt.errorbar(\n",
                "    x=low_variance_pairs_sorted[\"mean\"],\n",
                "    y=range(len(low_variance_pairs_sorted)),\n",
                "    xerr=low_variance_pairs_sorted[\"std\"],\n",
                "    fmt=\"o\",\n",
                "    capsize=5,\n",
                "    alpha=0.7,\n",
                "    label=\"Mean ± Std Dev\"\n",
                ")\n",
                "# Add min-max range\n",
                "for i, (_, row) in enumerate(low_variance_pairs_sorted.iterrows()):\n",
                "    plt.plot([row[\"min\"], row[\"max\"]], [i, i], \"k-\", alpha=0.5, label=\"Min-Max Range\" if i == 0 else \"\")\n",
                "plt.yticks(range(len(pair_labels)), pair_labels)\n",
                "plt.title(\"Language Pairs with Lowest Rate Variance\")\n",
                "plt.xlabel(\"Hourly Rate\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze translators by number of language pairs\n",
                "\n",
                "pair_count_quantiles = translator_lang_pair_counts[\"language_pair_count\"].quantile([0, 0.25, 0.5, 0.75, 0.9, 1]) # Calculate quantiles for language pair counts\n",
                "print(\"Language Pair Count Quantiles:\")\n",
                "print(pair_count_quantiles)\n",
                "\n",
                "# Create translator specialization categories\n",
                "trans_specialization_bins = [0, 1, 3, 6, 10, 100]  # Adjust based on your distribution\n",
                "trans_specialization_labels = [\n",
                "    \"Single Specialty (1)\", \n",
                "    \"Limited Range (2-3)\", \n",
                "    \"Moderate Range (4-6)\", \n",
                "    \"Wide Range (7-10)\", \n",
                "    \"Extensive Range (>10)\"\n",
                "]\n",
                "\n",
                "translator_lang_pair_counts[\"SPECIALIZATION\"] = pd.cut(\n",
                "    translator_lang_pair_counts[\"language_pair_count\"],\n",
                "    bins=trans_specialization_bins,\n",
                "    labels=trans_specialization_labels,\n",
                "    include_lowest=True\n",
                ")\n",
                "\n",
                "# Distribution of translator specialization\n",
                "specialization_counts = translator_lang_pair_counts[\"SPECIALIZATION\"].value_counts().sort_index()\n",
                "specialization_percent = specialization_counts / len(translator_lang_pair_counts) * 100\n",
                "\n",
                "print(\"\\nTranslator Specialization Distribution:\")\n",
                "for category, count in specialization_counts.items():\n",
                "    percentage = specialization_percent[category]\n",
                "    print(f\"  {category}: {count} translators ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize translator specialization\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.countplot(y=\"SPECIALIZATION\", data=translator_lang_pair_counts, \n",
                "                   order=specialization_counts.index, alpha=0.75)\n",
                "plt.title(\"Distribution of Translator Specialization\")\n",
                "plt.xlabel(\"Count of Translators\")\n",
                "plt.ylabel(\"Specialization Category\")\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / len(translator_lang_pair_counts)\n",
                "    ax.text(width + 5, p.get_y() + p.get_height()/2, \n",
                "            f\"{width} ({percentage:.1f}%)\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze relationship between specialization and hourly rates\n",
                "translator_avg_rates = df_translators.groupby(\"TRANSLATOR\")[\"HOURLY_RATE\"].mean().reset_index() # Calculate average hourly rate per translator\n",
                "translator_avg_rates.rename(columns={\"HOURLY_RATE\": \"AVG_HOURLY_RATE\"}, inplace=True)\n",
                "\n",
                "translator_analysis = pd.merge( # Merge with translator language pair counts\n",
                "    translator_lang_pair_counts,\n",
                "    translator_avg_rates,\n",
                "    on=\"TRANSLATOR\"\n",
                ")\n",
                "\n",
                "if \"SPECIALIZATION\" not in translator_analysis.columns:\n",
                "    # Define specialization categories based on language pair count\n",
                "    trans_specialization_bins = [0, 1, 3, 6, 10, 70]  # Boundaries for specialization categories\n",
                "    trans_specialization_labels = [\n",
                "        \"Single Specialty (1)\", \n",
                "        \"Limited Range (2-3)\", \n",
                "        \"Moderate Range (4-6)\", \n",
                "        \"Wide Range (7-10)\", \n",
                "        \"Extensive Range (>10)\"\n",
                "    ]\n",
                "    \n",
                "    # Create specialization categories\n",
                "    translator_analysis[\"SPECIALIZATION\"] = pd.cut(\n",
                "        translator_analysis[\"language_pair_count\"],\n",
                "        bins=trans_specialization_bins,\n",
                "        labels=trans_specialization_labels,\n",
                "        include_lowest=True\n",
                "    )\n",
                "\n",
                "# Calculate average rate by specialization category with observed=True to silence warning\n",
                "specialization_rates = translator_analysis.groupby(\"SPECIALIZATION\", observed=True).agg({\n",
                "    \"AVG_HOURLY_RATE\": [\"mean\", \"median\", \"std\", \"min\", \"max\", \"count\"]\n",
                "}).round(2)\n",
                "\n",
                "print(\"Average Hourly Rates by Specialization:\")\n",
                "display(specialization_rates)\n",
                "\n",
                "# Get specialization counts for proper ordering in plots\n",
                "specialization_counts = translator_analysis[\"SPECIALIZATION\"].value_counts().sort_index()\n",
                "\n",
                "# Visualize relationship between specialization and rates\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.boxplot(\n",
                "    x=\"SPECIALIZATION\", \n",
                "    y=\"AVG_HOURLY_RATE\", \n",
                "    data=translator_analysis.dropna(subset=[\"SPECIALIZATION\"]),  # Drop NaNs to avoid empty boxes\n",
                "    order=specialization_counts.index,\n",
                "\n",
                ")\n",
                "plt.title(\"Relationship Between Translator Specialization and Hourly Rates\")\n",
                "plt.xlabel(\"Specialization Category\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Create a scatter plot of pair count vs hourly rate\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.scatterplot(\n",
                "    x=\"language_pair_count\", \n",
                "    y=\"AVG_HOURLY_RATE\",\n",
                "    data=translator_analysis, \n",
                "    alpha=0.6\n",
                ")\n",
                "plt.title(\"Relationship Between Number of Language Pairs and Average Hourly Rate\")\n",
                "plt.xlabel(\"Number of Language Pairs\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "# Add a trend line\n",
                "sns.regplot(\n",
                "    x=\"language_pair_count\", \n",
                "    y=\"AVG_HOURLY_RATE\",\n",
                "    data=translator_analysis, \n",
                "    scatter=False,  # Don't add additional scatter points\n",
                "    line_kws={\"color\": \"red\"}\n",
                ")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.xlabel(\"Number of Language Pairs\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate correlation\n",
                "corr = translator_analysis[[\"language_pair_count\", \"AVG_HOURLY_RATE\"]].corr().iloc[0, 1]\n",
                "print(f\"Correlation between number of language pairs and average hourly rate: {corr:.4f}\")\n",
                "\n",
                "# Add additional insights about the relationship\n",
                "min_rate = translator_analysis.loc[translator_analysis[\"language_pair_count\"] == 1, \"AVG_HOURLY_RATE\"].mean()\n",
                "max_rate = translator_analysis.loc[translator_analysis[\"language_pair_count\"] > 10, \"AVG_HOURLY_RATE\"].mean()\n",
                "rate_increase = max_rate - min_rate\n",
                "percent_increase = (rate_increase / min_rate) * 100 if min_rate > 0 else 0\n",
                "\n",
                "print(f\"\\nAdditional Insights:\")\n",
                "print(f\"- Translators with only one language pair earn on average {min_rate:.2f}€/hour\")\n",
                "print(f\"- Translators with more than 10 language pairs earn on average {max_rate:.2f}€/hour\")\n",
                "print(f\"- This represents a difference of {rate_increase:.2f}€/hour or {percent_increase:.1f}% higher rates for diversified translators\")\n",
                "print(f\"- The positive correlation of {corr:.4f} suggests that specializing in more language pairs is associated with higher compensation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze language rarity and its impact on rates\n",
                "source_counts = df_translators[\"SOURCE_LANG\"].value_counts().reset_index() # Count translators for each language\n",
                "source_counts.columns = [\"LANGUAGE\", \"SOURCE_COUNT\"]\n",
                "target_counts = df_translators[\"TARGET_LANG\"].value_counts().reset_index()\n",
                "target_counts.columns = [\"LANGUAGE\", \"TARGET_COUNT\"]\n",
                "\n",
                "# Merge to get total counts\n",
                "language_counts = pd.merge(source_counts, target_counts, on=\"LANGUAGE\", how=\"outer\").fillna(0)\n",
                "language_counts[\"TOTAL_COUNT\"] = language_counts[\"SOURCE_COUNT\"] + language_counts[\"TARGET_COUNT\"]\n",
                "language_counts.sort_values(\"TOTAL_COUNT\", ascending=False, inplace=True)\n",
                "\n",
                "print(\"Most Common Languages (Source or Target):\")\n",
                "display(language_counts.head(15))\n",
                "\n",
                "print(\"\\nRarest Languages (Source or Target):\")\n",
                "display(language_counts.tail(15))\n",
                "\n",
                "# Calculate average rate for each language (when used as source or target)\n",
                "source_rates = df_translators.groupby(\"SOURCE_LANG\")[\"HOURLY_RATE\"].mean().reset_index()\n",
                "source_rates.columns = [\"LANGUAGE\", \"SOURCE_RATE\"]\n",
                "\n",
                "target_rates = df_translators.groupby(\"TARGET_LANG\")[\"HOURLY_RATE\"].mean().reset_index()\n",
                "target_rates.columns = [\"LANGUAGE\", \"TARGET_RATE\"]\n",
                "\n",
                "# Merge counts with rates\n",
                "language_analysis = pd.merge(language_counts, source_rates, on=\"LANGUAGE\", how=\"left\")\n",
                "language_analysis = pd.merge(language_analysis, target_rates, on=\"LANGUAGE\", how=\"left\")\n",
                "\n",
                "# Calculate weighted average rate\n",
                "language_analysis[\"AVG_RATE\"] = (\n",
                "    (language_analysis[\"SOURCE_RATE\"] * language_analysis[\"SOURCE_COUNT\"] + \n",
                "     language_analysis[\"TARGET_RATE\"] * language_analysis[\"TARGET_COUNT\"]) / \n",
                "    language_analysis[\"TOTAL_COUNT\"]\n",
                ")\n",
                "\n",
                "# Create rarity categories\n",
                "rarity_bins = [0, 10, 50, 100, 200, 500, 10000]\n",
                "rarity_labels = [\"Very Rare (<10)\", \"Rare (10-50)\", \"Uncommon (51-100)\", \n",
                "                \"Common (101-200)\", \"Very Common (201-500)\", \"Abundant (>500)\"]\n",
                "\n",
                "language_analysis[\"RARITY\"] = pd.cut(\n",
                "    language_analysis[\"TOTAL_COUNT\"],\n",
                "    bins=rarity_bins,\n",
                "    labels=rarity_labels,\n",
                "    include_lowest=True\n",
                ")\n",
                "\n",
                "# Analyze relationship between language rarity and rates\n",
                "rarity_rates = language_analysis.groupby(\"RARITY\").agg({\n",
                "    \"AVG_RATE\": [\"mean\", \"median\", \"std\", \"count\"]\n",
                "}).round(2)\n",
                "\n",
                "print(\"\\nRelationship Between Language Rarity and Rates:\")\n",
                "display(rarity_rates)\n",
                "\n",
                "# Visualize relationship\n",
                "plt.figure(figsize=(14, 8))\n",
                "rarity_means = language_analysis.groupby(\"RARITY\")[\"AVG_RATE\"].mean().sort_index()\n",
                "ax = sns.barplot(x=rarity_means.index, y=rarity_means.values, alpha=0.75)\n",
                "plt.title(\"Average Hourly Rate by Language Rarity\")\n",
                "plt.xlabel(\"Language Rarity\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "\n",
                "# Add rate labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    height = p.get_height()\n",
                "    ax.text(p.get_x() + p.get_width()/2, height + 0.5, \n",
                "            f\"{height:.2f}€\", ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Scatter plot of language count vs. rate\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.scatter(language_analysis[\"TOTAL_COUNT\"], language_analysis[\"AVG_RATE\"], alpha=0.6)\n",
                "plt.title(\"Relationship Between Language Frequency and Average Rate\")\n",
                "plt.xlabel(\"Language Frequency (Total Count)\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.xscale(\"log\")  # Use log scale for better visualization\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "# Add labels for some points\n",
                "for idx, row in language_analysis.iterrows():\n",
                "    if row[\"AVG_RATE\"] > 40 or row[\"TOTAL_COUNT\"] < 5 or row[\"TOTAL_COUNT\"] > 500:\n",
                "        plt.annotate(row[\"LANGUAGE\"], \n",
                "                    (row[\"TOTAL_COUNT\"], row[\"AVG_RATE\"]),\n",
                "                    xytext=(5, 5),\n",
                "                    textcoords=\"offset points\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze language direction premium (source vs. target)\n",
                "direction_analysis = pd.merge(  # For languages that appear as both source and target\n",
                "    source_rates, \n",
                "    target_rates, \n",
                "    on=\"LANGUAGE\", \n",
                "    how=\"inner\",\n",
                "    suffixes=(\"_SOURCE\", \"_TARGET\")\n",
                ")\n",
                "\n",
                "# Calculate premium (positive if target rate > source rate)\n",
                "direction_analysis[\"DIRECTION_PREMIUM\"] = direction_analysis[\"TARGET_RATE\"] - direction_analysis[\"SOURCE_RATE\"]\n",
                "direction_analysis[\"PREMIUM_PCT\"] = (direction_analysis[\"DIRECTION_PREMIUM\"] / direction_analysis[\"SOURCE_RATE\"]) * 100\n",
                "\n",
                "# Merge with language counts for context\n",
                "direction_analysis = pd.merge(\n",
                "    direction_analysis,\n",
                "    language_counts[[\"LANGUAGE\", \"TOTAL_COUNT\"]],\n",
                "    on=\"LANGUAGE\"\n",
                ")\n",
                "\n",
                "# Filter to languages with reasonable frequency\n",
                "min_freq = 10\n",
                "direction_filtered = direction_analysis[direction_analysis[\"TOTAL_COUNT\"] >= min_freq].copy()\n",
                "\n",
                "# Sort by premium\n",
                "source_premium = direction_filtered.sort_values(\"DIRECTION_PREMIUM\", ascending=False)\n",
                "target_premium = direction_filtered.sort_values(\"DIRECTION_PREMIUM\", ascending=True)\n",
                "\n",
                "print(f\"Top Languages with Source > Target Premium (min freq {min_freq}):\")\n",
                "display(source_premium.head(15))\n",
                "\n",
                "print(f\"\\nTop Languages with Target > Source Premium (min freq {min_freq}):\")\n",
                "display(target_premium.head(15))\n",
                "\n",
                "# Visualize direction premium\n",
                "plt.figure(figsize=(16, 10))\n",
                "combined_premium = pd.concat([source_premium.head(10), target_premium.head(10)])\n",
                "combined_premium = combined_premium.drop_duplicates(subset=[\"LANGUAGE\"])\n",
                "combined_premium = combined_premium.sort_values(\"DIRECTION_PREMIUM\")\n",
                "\n",
                "ax = sns.barplot(\n",
                "    x=\"DIRECTION_PREMIUM\", \n",
                "    y=\"LANGUAGE\",\n",
                "    data=combined_premium,\n",
                "    palette=\"coolwarm_r\",  # Red for negative, blue for positive\n",
                "    alpha=0.75\n",
                ")\n",
                "plt.title(\"Hourly Rate Difference: Translating To vs. From Languages\")\n",
                "plt.xlabel(\"Hourly Rate Difference (€)\")\n",
                "plt.ylabel(\"Language\")\n",
                "plt.axvline(x=0, color=\"black\", linestyle=\"--\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "# Add improved labels without percentage\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    abs_width = abs(width)\n",
                "    label = f\"€{abs_width:.2f} T>S\" if width >= 0 else f\"€{abs_width:.2f} S>T\"\n",
                "    label_pos = width - 2 if width >= 0 else width + 2\n",
                "    ax.text(label_pos, p.get_y() + p.get_height()/2, \n",
                "            label, ha=\"left\" if width >= 0 else \"right\", va=\"center\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cluster analysis for language pairs\n",
                "\n",
                "cluster_data = avg_rates_by_pair.copy() # Use language pair frequency and rate information\n",
                "\n",
                "# Simplify data for clustering\n",
                "cluster_features = [\n",
                "    \"mean\",   # Average hourly rate\n",
                "    \"std\",    # Standard deviation of rates\n",
                "    \"count\"   # Number of translators\n",
                "]\n",
                "\n",
                "min_trans = 3 # Filter to pairs with at least 3 translators for meaningful analysis\n",
                "cluster_data = cluster_data[cluster_data[\"count\"] >= min_trans]\n",
                "\n",
                "# Standardize data for KMeans\n",
                "cluster_data_scaled = (cluster_data[cluster_features] - cluster_data[cluster_features].mean()) / \\\n",
                "                       cluster_data[cluster_features].std()\n",
                "\n",
                "# Determine optimal number of clusters\n",
                "inertia = []\n",
                "k_range = range(1, 10)\n",
                "for k in k_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    kmeans.fit(cluster_data_scaled)\n",
                "    inertia.append(kmeans.inertia_)\n",
                "\n",
                "# Plot elbow method\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.plot(k_range, inertia, marker=\"o\")\n",
                "plt.title(\"Elbow Method for Language Pair Cluster Selection\")\n",
                "plt.xlabel(\"Number of Clusters\")\n",
                "plt.ylabel(\"Inertia\")\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimal_k = 4  # Adjust based on the elbow plot\n",
                "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
                "cluster_data[\"CLUSTER\"] = kmeans.fit_predict(cluster_data_scaled)\n",
                "\n",
                "# Analyze the clusters\n",
                "cluster_stats = cluster_data.groupby(\"CLUSTER\").agg({\n",
                "    \"mean\": [\"mean\", \"min\", \"max\"],\n",
                "    \"std\": [\"mean\"],\n",
                "    \"count\": [\"mean\", \"min\", \"max\", \"sum\"],\n",
                "    \"SOURCE_LANG\": \"count\"\n",
                "}).round(2)\n",
                "\n",
                "# Rename columns for better readability\n",
                "cluster_stats.columns = [\n",
                "    \"Avg_Rate_Mean\", \"Avg_Rate_Min\", \"Avg_Rate_Max\", \n",
                "    \"Std_Dev_Mean\", \n",
                "    \"Translators_Mean\", \"Translators_Min\", \"Translators_Max\", \"Total_Translators\",\n",
                "    \"Lang_Pair_Count\"\n",
                "]\n",
                "\n",
                "print(\"Language Pair Cluster Statistics:\")\n",
                "display(cluster_stats)\n",
                "\n",
                "# Calculate percentage by cluster\n",
                "cluster_counts = cluster_data[\"CLUSTER\"].value_counts().sort_index()\n",
                "cluster_percentages = cluster_counts / len(cluster_data) * 100\n",
                "\n",
                "print(\"\\nCluster Distribution:\")\n",
                "for cluster, count in cluster_counts.items():\n",
                "    percentage = cluster_percentages[cluster]\n",
                "    print(f\"Cluster {cluster}: {count} language pairs ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize the clusters\n",
                "plt.figure(figsize=(14, 8))\n",
                "scatter = plt.scatter(\n",
                "    cluster_data[\"count\"],\n",
                "    cluster_data[\"mean\"],\n",
                "    c=cluster_data[\"CLUSTER\"],\n",
                "    cmap=\"YlGnBu\",\n",
                "    s=100,\n",
                "    alpha=0.6\n",
                ")\n",
                "plt.colorbar(scatter, label=\"Cluster\")\n",
                "plt.title(\"Language Pair Clusters by Translator Count and Average Rate\")\n",
                "plt.xlabel(\"Number of Translators\")\n",
                "plt.ylabel(\"Average Hourly Rate\")\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "\n",
                "# Add cluster centroids\n",
                "centroids = kmeans.cluster_centers_\n",
                "centroids_unscaled = centroids * cluster_data[cluster_features].std().values + \\\n",
                "                    cluster_data[cluster_features].mean().values\n",
                "plt.scatter(\n",
                "    centroids_unscaled[:, 2],  # count feature\n",
                "    centroids_unscaled[:, 0],  # mean feature\n",
                "    s=50,\n",
                "    marker=\"X\",\n",
                "    c=\"red\",\n",
                "    label=\"Centroids\"\n",
                ")\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Assign cluster labels based on characteristics\n",
                "cluster_labels = {\n",
                "    # Customize based on actual clusters\n",
                "    0: \"Premium Specialized\",\n",
                "    1: \"Standard Common\",\n",
                "    2: \"Budget Common\", \n",
                "    3: \"Rare Specialty\"\n",
                "}\n",
                "\n",
                "# Map cluster numbers to labels\n",
                "cluster_data[\"CLUSTER_LABEL\"] = cluster_data[\"CLUSTER\"].map(cluster_labels)\n",
                "\n",
                "# Sample of language pairs in each cluster\n",
                "print(\"\\nSample Language Pairs by Cluster:\")\n",
                "for cluster, label in cluster_labels.items():\n",
                "    cluster_sample = cluster_data[cluster_data[\"CLUSTER\"] == cluster].head(5)\n",
                "    print(f\"\\nCluster {cluster}: {label}\")\n",
                "    for _, row in cluster_sample.iterrows():\n",
                "        print(f\"  {row['SOURCE_LANG']} > {row['TARGET_LANG']}: €{row['mean']:.2f} avg rate, {row['count']} translators\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate average rates for high-demand language pairs\n",
                "top_pairs = pair_languages_top.copy()  # Use the top most common pairs from previous analysis\n",
                "\n",
                "# Merge with rate information\n",
                "top_pairs_with_rates = pd.merge(\n",
                "    top_pairs,\n",
                "    avg_rates_by_pair[[\"SOURCE_LANG\", \"TARGET_LANG\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"count\"]],\n",
                "    on=[\"SOURCE_LANG\", \"TARGET_LANG\"]\n",
                ")\n",
                "\n",
                "# Sort by count descending and show rate information\n",
                "top_pairs_with_rates = top_pairs_with_rates.sort_values(\"COUNT\", ascending=False)\n",
                "print(\"Rate Information for Top Most In-Demand Language Pairs:\")\n",
                "display(top_pairs_with_rates)\n",
                "\n",
                "# Calculate total cost for top pairs\n",
                "top_pairs_with_rates[\"TOTAL_COST\"] = top_pairs_with_rates[\"COUNT\"] * top_pairs_with_rates[\"mean\"]\n",
                "top_pairs_with_rates[\"COST_PERCENTAGE\"] = (top_pairs_with_rates[\"TOTAL_COST\"] / top_pairs_with_rates[\"TOTAL_COST\"].sum()) * 100\n",
                "\n",
                "print(\"\\nCost Analysis for Top Language Pairs:\")\n",
                "print(f\"Total cost for Top pairs: {top_pairs_with_rates['TOTAL_COST'].sum():.2f}€\")\n",
                "\n",
                "# Visualize cost distribution for top pairs\n",
                "plt.figure(figsize=(16, 10))\n",
                "top_10_cost = top_pairs_with_rates.head(10).copy()\n",
                "top_10_cost[\"PAIR_LABEL\"] = top_10_cost.apply(\n",
                "    lambda x: f\"{x['SOURCE_LANG']} $\\\\rightarrow$ {x['TARGET_LANG']} (€{x['mean']:.2f})\", axis=1\n",
                ")\n",
                "\n",
                "# Generate colors from YlGnBu colormap\n",
                "cmap = cm.get_cmap(\"YlGnBu\")\n",
                "colors = [cmap(i / 10) for i in range(10)]\n",
                "\n",
                "plt.pie(\n",
                "    top_10_cost[\"TOTAL_COST\"], \n",
                "    labels=top_10_cost[\"PAIR_LABEL\"],\n",
                "    autopct=\"%1.1f%%\",\n",
                "    startangle=90,\n",
                "    shadow=False,\n",
                "    explode=[0.1 if i == 0 else 0.05 if i == 1 else 0 for i in range(10)],\n",
                "    colors=colors\n",
                ")\n",
                "plt.title(\"Cost Distribution Among top Language Pairs\")\n",
                "plt.axis(\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze rate competitiveness and translator distribution\n",
                "df_translators[\"RATE_ZSCORE\"] = (\n",
                "    df_translators[\"HOURLY_RATE\"] - df_translators[\"HOURLY_RATE\"].mean()\n",
                ") / df_translators[\"HOURLY_RATE\"].std()\n",
                "\n",
                "conditions = [ # Create rate competitiveness categories\n",
                "    (df_translators[\"RATE_ZSCORE\"] >= -2) & (df_translators[\"RATE_ZSCORE\"] < -1),\n",
                "    (df_translators[\"RATE_ZSCORE\"] >= -1) & (df_translators[\"RATE_ZSCORE\"] < 1),\n",
                "    (df_translators[\"RATE_ZSCORE\"] >= 1) & (df_translators[\"RATE_ZSCORE\"] < 2),\n",
                "    (df_translators[\"RATE_ZSCORE\"] >= 2)\n",
                "]\n",
                "\n",
                "comp_values = [\"Competitive\", \"Average\", \"Premium\", \"Very Premium\"]\n",
                "df_translators[\"COMPETITIVENESS\"] = np.select(conditions, comp_values, default=\"Average\")\n",
                "\n",
                "# Competitiveness distribution - explicitly define the order to ensure consistency\n",
                "competitiveness_order = [\"Competitive\", \"Average\", \"Premium\", \"Very Premium\"]\n",
                "comp_counts = df_translators[\"COMPETITIVENESS\"].value_counts()\n",
                "# Reindex to ensure all categories are present with consistent order\n",
                "comp_counts = comp_counts.reindex(competitiveness_order, fill_value=0)\n",
                "comp_percent = comp_counts / len(df_translators) * 100\n",
                "\n",
                "print(\"Rate Competitiveness Distribution:\")\n",
                "for comp in competitiveness_order:\n",
                "    count = comp_counts[comp]\n",
                "    percentage = comp_percent[comp]\n",
                "    print(f\"  {comp}: {count} translator rates ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize rate competitiveness - use the same explicit order\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.countplot(\n",
                "    x=\"COMPETITIVENESS\", \n",
                "    data=df_translators, \n",
                "    order=competitiveness_order,\n",
                "    alpha=0.75\n",
                ")\n",
                "plt.title(\"Distribution of Rate Competitiveness\")\n",
                "plt.xlabel(\"Competitiveness Category\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\")\n",
                "for i, p in enumerate(plt.gca().patches): # Add count labels\n",
                "    width = p.get_height()\n",
                "    plt.text(p.get_x() + p.get_width()/2, width + 5, \n",
                "             f\"{width:.0f}\", ha=\"center\", va=\"bottom\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Analyze competitiveness by language pair\n",
                "comp_by_pair = df_translators.groupby([\"SOURCE_LANG\", \"TARGET_LANG\"])[\"COMPETITIVENESS\"].value_counts(normalize=True)\n",
                "comp_by_pair = comp_by_pair.mul(100).rename(\"PERCENTAGE\").reset_index()\n",
                "\n",
                "# Identify language pairs with most competitive rates - being explicit about the categories\n",
                "competitive_categories = [\"Very Competitive\", \"Competitive\"]\n",
                "top_competitive_pairs = comp_by_pair[\n",
                "    (comp_by_pair[\"COMPETITIVENESS\"].isin(competitive_categories)) &\n",
                "    (comp_by_pair[\"PERCENTAGE\"] > 50)  # At least 50% of translators have competitive rates\n",
                "].sort_values(\"PERCENTAGE\", ascending=False)\n",
                "\n",
                "print(\"\\nLanguage Pairs with Most Competitive Rates:\")\n",
                "display(top_competitive_pairs.head(10))\n",
                "\n",
                "# Identify language pairs with most premium rates - being explicit about the categories\n",
                "premium_categories = [\"Premium\", \"Very Premium\"]\n",
                "top_premium_pairs = comp_by_pair[\n",
                "    (comp_by_pair[\"COMPETITIVENESS\"].isin(premium_categories)) &\n",
                "    (comp_by_pair[\"PERCENTAGE\"] > 50)  # At least 50% of translators have premium rates\n",
                "].sort_values(\"PERCENTAGE\", ascending=False)\n",
                "\n",
                "print(\"\\nLanguage Pairs with Most Premium Rates:\")\n",
                "display(top_premium_pairs.head(10))\n",
                "\n",
                "# Calculate the total counts and percentages for key categories\n",
                "competitive_count = comp_counts[\"Competitive\"]\n",
                "competitive_percent = comp_percent[\"Competitive\"]\n",
                "premium_count = comp_counts[\"Premium\"] + comp_counts[\"Very Premium\"]\n",
                "premium_percent = comp_percent[\"Premium\"] + comp_percent[\"Very Premium\"]\n",
                "\n",
                "# Print a summary for easy reference\n",
                "print(\"\\nSummary statistics:\")\n",
                "print(f\"Competitive rates: {competitive_count} ({competitive_percent:.2f}%)\")\n",
                "print(f\"Premium rates: {premium_count} ({premium_percent:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"KEY FINDINGS FROM TRANSLATOR COST PAIRS ANALYSIS:\")\n",
                "print()\n",
                "print(\"Translator Pool Size:\")\n",
                "print(f\"    {df_translators['TRANSLATOR'].nunique()} unique translators across {df_translators.groupby(['SOURCE_LANG', 'TARGET_LANG']).ngroups} distinct language pairs\")\n",
                "print()\n",
                "print(\"Language Coverage:\")\n",
                "print(f\"    Source languages: {df_translators['SOURCE_LANG'].nunique()}\")\n",
                "print(f\"    Target languages: {df_translators['TARGET_LANG'].nunique()}\")\n",
                "print()\n",
                "print(\"Rate Distribution:\")\n",
                "print(f\"    Average hourly rate: {df_translators['HOURLY_RATE'].mean():.2f}€\")\n",
                "print(f\"    Rate range: {df_translators['HOURLY_RATE'].min():.2f}€ to {df_translators['HOURLY_RATE'].max():.2f}€\")\n",
                "print(f\"    Median rate: {df_translators['HOURLY_RATE'].median():.2f}€\")\n",
                "print()\n",
                "print(\"Specialization Patterns:\")\n",
                "print(f\"    Single language pair specialists: {len(translator_lang_pair_counts[translator_lang_pair_counts['language_pair_count'] == 1])} translators ({len(translator_lang_pair_counts[translator_lang_pair_counts['language_pair_count'] == 1])/len(translator_lang_pair_counts)*100:.2f}%)\")\n",
                "print(f\"    Highly versatile translators (>10 pairs): {len(translator_lang_pair_counts[translator_lang_pair_counts['language_pair_count'] > 10])} translators ({len(translator_lang_pair_counts[translator_lang_pair_counts['language_pair_count'] > 10])/len(translator_lang_pair_counts)*100:.2f}%)\")\n",
                "print()\n",
                "print(\"Rate Variation:\")\n",
                "print(f\"    Correlation between language pairs and rate: {corr:.4f}\")\n",
                "print()\n",
                "print(\"Language Rarity Premium:\")\n",
                "print(f\"    Very rare languages: {language_analysis[language_analysis['RARITY'] == 'Very Rare (<10)']['AVG_RATE'].mean():.2f}€\")\n",
                "print(f\"    Abundant languages: {language_analysis[language_analysis['RARITY'] == 'Abundant (>500)']['AVG_RATE'].mean():.2f}€\")\n",
                "print()\n",
                "print(\"Popular Language Pairs:\")\n",
                "print(f\"    Most common: {pair_languages_top.iloc[0]['SOURCE_LANG']} → {pair_languages_top.iloc[0]['TARGET_LANG']} with {pair_languages_top.iloc[0]['COUNT']} translators\")\n",
                "print(f\"    Second most common: {pair_languages_top.iloc[1]['SOURCE_LANG']} → {pair_languages_top.iloc[1]['TARGET_LANG']} with {pair_languages_top.iloc[1]['COUNT']} translators\")\n",
                "print()\n",
                "print(\"Rare Language Pairs:\")\n",
                "print(f\"    Single-translator pairs: {len(pair_languages[pair_languages['COUNT'] == 1])} ({len(pair_languages[pair_languages['COUNT'] == 1])/len(pair_languages)*100:.2f}% of all pairs)\")\n",
                "print()\n",
                "print(\"Direction Premium:\")\n",
                "print(f\"    Highest premium: {direction_analysis.iloc[direction_analysis['DIRECTION_PREMIUM'].argmax()]['LANGUAGE']} at {direction_analysis.iloc[direction_analysis['DIRECTION_PREMIUM'].argmax()]['DIRECTION_PREMIUM']:.2f}€ when used as target language\")\n",
                "print()\n",
                "print(\"Rate Clusters:\")\n",
                "print(f\"    Distinct rate clusters: {optimal_k}\")\n",
                "print(f\"    Largest cluster: {cluster_counts.max()} pairs ({cluster_percentages.max():.2f}% of all pairs)\")\n",
                "print()\n",
                "print(\"Rate Competitiveness:\")\n",
                "print(f\"    Competitive rates: {comp_counts['Competitive']} ({(comp_percent['Competitive']):.2f}%)\")\n",
                "print(f\"    Premium/very premium rates: {comp_counts['Premium'] + comp_counts['Very Premium']} ({(comp_percent['Premium'] + comp_percent['Very Premium']):.2f}%)\")\n",
                "print()\n",
                "print(\"High-Value Language Pairs:\")\n",
                "print(f\"    Top pairs by volume: {top_10_cost['COST_PERCENTAGE'].sum():.2f}% of total translation cost\")\n",
                "print(f\"    Highest volume pair: {top_10_cost.iloc[0]['SOURCE_LANG']} → {top_10_cost.iloc[0]['TARGET_LANG']} ({top_10_cost.iloc[0]['COST_PERCENTAGE']:.2f}% of cost)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### CSV Sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Basic information:\")\n",
                "df_data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Missing values by column:\")\n",
                "print(df_data.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert timestamp columns to datetime\n",
                "timestamp_cols = [\"START\", \"END\", \"ASSIGNED\", \"READY\", \"WORKING\", \"DELIVERED\", \"RECEIVED\", \"CLOSE\"]\n",
                "for col in timestamp_cols:\n",
                "    if col in df_data.columns:\n",
                "        df_data[col] = pd.to_datetime(df_data[col], errors=\"coerce\")\n",
                "\n",
                "print(\"Data types after conversion:\")\n",
                "print(df_data.dtypes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the first few rows\n",
                "print(\"Sample data:\")\n",
                "display(df_data.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display basic information\n",
                "print(f\"Number of rows: {df_data.shape[0]}\")\n",
                "print(f\"Number of columns: {df_data.shape[1]}\")\n",
                "print(f\"Number of unique projects: {df_data['PROJECT_ID'].nunique()}\")\n",
                "print(f\"Number of unique tasks: {df_data['TASK_ID'].nunique()}\")\n",
                "print(f\"Number of unique translators: {df_data['TRANSLATOR'].nunique()}\")\n",
                "print(f\"Number of unique project managers: {df_data['PM'].nunique()}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DONE\n",
                "# Duplicate values of TASK_ID exists\n",
                "# - This might be an issue with the data collection\n",
                "# - Or simply a different project with the same task id\n",
                "# - Time information indicates that these are different tasks but with the same id\n",
                "\n",
                "df_data[df_data[\"TASK_ID\"].duplicated()]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count unique values for categorical columns\n",
                "categorical_cols = [\"PM\", \"TASK_TYPE\", \"SOURCE_LANG\", \"TARGET_LANG\", \n",
                "                   \"TRANSLATOR\", \"MANUFACTURER\", \"MANUFACTURER_SECTOR\",\n",
                "                   \"MANUFACTURER_INDUSTRY_GROUP\", \"MANUFACTURER_INDUSTRY\", \n",
                "                   \"MANUFACTURER_SUBINDUSTRY\"]\n",
                "\n",
                "print(\"Unique values for categorical columns:\")\n",
                "for col in categorical_cols:\n",
                "    print(f\"{col}: {df_data[col].nunique()} unique values\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Columns with repetitive information\n",
                "df_data[[\"MANUFACTURER\", \"MANUFACTURER_SECTOR\", \"MANUFACTURER_INDUSTRY_GROUP\", \"MANUFACTURER_INDUSTRY\", \"MANUFACTURER_SUBINDUSTRY\"]].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic information\n",
                "print(f\"Number of unique projects: {df_data['PROJECT_ID'].nunique()}\")\n",
                "print(f\"Number of unique tasks: {df_data['TASK_ID'].nunique()}\")\n",
                "print(f\"Number of unique translators: {df_data['TRANSLATOR'].nunique()}\")\n",
                "print(f\"Number of source languages: {df_data['SOURCE_LANG'].nunique()}\")\n",
                "print(f\"Number of target languages: {df_data['TARGET_LANG'].nunique()}\")\n",
                "print(f\"Number of unique task types: {df_data['TASK_TYPE'].nunique()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics for numeric columns\n",
                "print(\"Summary statistics for numeric columns:\")\n",
                "numeric_cols = df_data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
                "display(df_data[numeric_cols].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check distribution of quality evaluations (quality control scores)\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_data[\"QUALITY_EVALUATION\"].dropna(), bins=10, alpha=0.75)\n",
                "plt.title(\"Distribution of Quality Evaluations\")\n",
                "plt.xlabel(\"Quality Score\")\n",
                "plt.ylabel(\"Count of Tasks\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Translator hourly compensation rates\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_data[\"HOURLY_RATE\"].dropna(), kde=True, alpha=0.75)\n",
                "plt.title(\"Translator Hourly Compensation Rates\")\n",
                "plt.xlabel(\"Hourly Rate\")\n",
                "plt.ylabel(\"Number of Translators\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Total project costs\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(df_data[\"COST\"].dropna(), kde=False, alpha=0.75)\n",
                "plt.title(\"Distribution of Project Costs\")\n",
                "plt.xlim(0, 150)\n",
                "plt.xlabel(\"Cost\")\n",
                "plt.ylabel(\"Number of Projects\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Indicate the top projects with higher task counts\n",
                "top_projects = df_data[\"PROJECT_ID\"].value_counts().head(10)\n",
                "print(\"Top projects with higher task counts:\")\n",
                "for project, count in top_projects.items():\n",
                "    print(f\"\\t{project}: {count} tasks\")\n",
                "\n",
                "# Indicate the lowest projects with lower task counts\n",
                "bottom_projects = df_data[\"PROJECT_ID\"].value_counts().tail(10)\n",
                "print(\"\\nLowest projects with lower task counts:\")\n",
                "for project, count in bottom_projects.items():\n",
                "    print(f\"\\t{project}: {count} tasks\")\n",
                "\n",
                "\n",
                "# Analyze tasks per project\n",
                "tasks_per_project = df_data.groupby(\"PROJECT_ID\").size()\n",
                "print(\"\\nTasks per Project Statistics:\")\n",
                "print(tasks_per_project.describe())\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.histplot(tasks_per_project, bins=1000, kde=True, edgecolor=\"black\", alpha=0.75)\n",
                "plt.title(\"Distribution of Tasks per Project\")\n",
                "plt.xlabel(\"Number of Tasks\")\n",
                "plt.ylabel(\"Count of Projects\")\n",
                "plt.xlim(0, 100)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPORTANT\n",
                "\n",
                "# Ranges of taks per project to get an idea of the distribution\n",
                "bins = [0, 2, 5, 10, 20, 35, 50, 100, 150, 200, 500, 1000, 2000, 5000, 10000, float(\"inf\")]\n",
                "labels = [\"0-2\", \"2-5\", \"5-10\", \"10-20\", \"20-35\", \"35-50\", \"50-100\", \"100-150\", \"150-200\", \"200-500\", \"500-1000\", \"1000-2000\", \"2000-5000\", \"5000-10000\", \">10000\"]\n",
                "\n",
                "# Bin tasks_per_project\n",
                "task_ranges = pd.cut(\n",
                "    tasks_per_project,\n",
                "    bins=bins,\n",
                "    labels=labels,\n",
                "    right=False\n",
                ")\n",
                "\n",
                "task_range_counts = task_ranges.value_counts().sort_index() # Count projects in each range\n",
                "task_range_percentages = task_range_counts / len(tasks_per_project) * 100 # Calculate percentages\n",
                "\n",
                "# Display the results\n",
                "print(\"Task Range Distribution:\")\n",
                "for range_label, count in task_range_counts.items():\n",
                "    percentage = task_range_percentages[range_label]\n",
                "    print(f\"  {range_label}: {count} projects ({percentage:.2f}%)\")\n",
                "\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.bar(task_range_counts.index, task_range_counts.values, alpha=0.75)\n",
                "plt.title(\"Distribution of Tasks per Project Ranges\")\n",
                "plt.xlabel(\"Task Range\")\n",
                "plt.ylabel(\"Number of Projects\")\n",
                "plt.xticks(rotation=45)\n",
                "# Add count labels\n",
                "for i, count in enumerate(task_range_counts):\n",
                "    plt.text(i, count + 5, str(count), ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze project distribution\n",
                "projects_per_pm = df_data.groupby(\"PM\")[\"PROJECT_ID\"].nunique()\n",
                "tasks_per_pm = df_data.groupby(\"PM\").size()\n",
                "# print(tasks_per_pm)\n",
                "\n",
                "pm_stats = pd.DataFrame({\n",
                "    \"Projects\": projects_per_pm,\n",
                "    \"Tasks\": tasks_per_pm,\n",
                "    \"Avg Tasks/Project\": tasks_per_pm / projects_per_pm\n",
                "}).sort_values(\"Tasks\", ascending=True)\n",
                "\n",
                "print(\"Project Manager Workload:\")\n",
                "display(pm_stats)\n",
                "\n",
                "# Visualize PM workload\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.barplot(x=pm_stats.head(15).index, y=pm_stats.head(15)[\"Tasks\"], alpha=0.75)\n",
                "plt.title(\"Top Project Managers by Number of Tasks\")\n",
                "plt.xlabel(\"Project Manager\")\n",
                "plt.ylabel(\"Number of Tasks\")\n",
                "# Add count labels\n",
                "for i, count in enumerate(pm_stats.head(15)[\"Tasks\"]):\n",
                "    plt.text(i, count + 5, str(count), ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task Type Analysis\n",
                "task_type_counts = df_data[\"TASK_TYPE\"].value_counts()\n",
                "task_type_percentage = task_type_counts / len(df_data) * 100\n",
                "\n",
                "print(\"Task Type Distribution:\")\n",
                "for task_type, count in task_type_counts.items():\n",
                "    percentage = task_type_percentage[task_type]\n",
                "    print(f\"  {task_type}: {count} tasks ({percentage:.2f}%)\")\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "sns.countplot(y=\"TASK_TYPE\", data=df_data, order=task_type_counts.index, alpha=0.75)\n",
                "plt.ylabel(\"Translation Service Category\")\n",
                "plt.xlabel(\"Number of Tasks\")\n",
                "plt.title(\"Distribution of Translation Service Categories\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in [\"ASSIGNED\", \"DELIVERED\"]: # Ensure date columns are in datetime format\n",
                "    if not pd.api.types.is_datetime64_dtype(df_data[col]):\n",
                "        df_data[col] = pd.to_datetime(df_data[col], errors=\"coerce\")\n",
                "\n",
                "# Calculate raw timedelta (difference between DELIVERED and ASSIGNED)\n",
                "mask = (~df_data[\"ASSIGNED\"].isnull()) & (~df_data[\"DELIVERED\"].isnull())\n",
                "df_data.loc[mask, \"TURNAROUND_TIME\"] = df_data.loc[mask, \"DELIVERED\"] - df_data.loc[mask, \"ASSIGNED\"]\n",
                "df_data_filtered = df_data.copy() # Keep changes in a new DataFrame\n",
                "\n",
                "# Filter out unreasonable values (Negative turnaround times)\n",
                "reasonable_mask = df_data_filtered[\"TURNAROUND_TIME\"].dt.total_seconds() > 0\n",
                "df_data_filtered = df_data_filtered.loc[reasonable_mask].sort_values(\"TURNAROUND_TIME\") # DataFrame sorted by turnaround time\n",
                "\n",
                "\n",
                "# Display shortest and longest tasks\n",
                "print(\"Shortest 5 tasks by turnaround time:\")\n",
                "display(df_data_filtered[[\"ASSIGNED\", \"DELIVERED\", \"TURNAROUND_TIME\"]].head(5))\n",
                "print(\"Longest 5 tasks by turnaround time:\")\n",
                "display(df_data_filtered[[\"ASSIGNED\", \"DELIVERED\", \"TURNAROUND_TIME\"]].tail(5))\n",
                "\n",
                "# Visualize distribution of turnaround times (Transformed to hours)\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.hist(df_data_filtered[\"TURNAROUND_TIME\"].dt.total_seconds() / 3600, bins=1000, edgecolor=\"black\", alpha=0.75)\n",
                "plt.title(\"Distribution of Tasks Turnaround Times\")\n",
                "plt.xlabel(\"Turnaround Time (Hours)\")\n",
                "plt.ylabel(\"Count of Tasks\")\n",
                "plt.xlim(0, (df_data_filtered[\"TURNAROUND_TIME\"].dt.total_seconds() / 3600).quantile(0.95))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate turnaround time and hours safely\n",
                "for col in [\"ASSIGNED\", \"DELIVERED\"]:\n",
                "    if df_data[col].dtype != \"datetime64[ns]\":\n",
                "        df_data[col] = pd.to_datetime(df_data[col], errors=\"coerce\")\n",
                "\n",
                "# Create turnaround time column\n",
                "mask = (~df_data[\"ASSIGNED\"].isna() & ~df_data[\"DELIVERED\"].isna())\n",
                "df_data.loc[mask, \"TURNAROUND_TIME\"] = df_data.loc[mask, \"DELIVERED\"] - df_data.loc[mask, \"ASSIGNED\"]\n",
                "\n",
                "# Filter for reasonable values (positive turnaround time)\n",
                "reasonable_mask = mask & (df_data[\"TURNAROUND_TIME\"].dt.total_seconds() > 0)\n",
                "\n",
                "# Calculate hours\n",
                "df_data.loc[reasonable_mask, \"TURNAROUND_HOURS\"] = df_data.loc[reasonable_mask, \"TURNAROUND_TIME\"].dt.total_seconds() / 3600\n",
                "\n",
                "# Calculate description turnaround time by translator\n",
                "df_translator_turnaround = df_data.loc[reasonable_mask].groupby(\"TRANSLATOR\")[\"TURNAROUND_HOURS\"].agg(\n",
                "    [\"median\", \"mean\", \"std\", \"count\"]\n",
                ").reset_index()\n",
                "\n",
                "df_translator_turnaround = df_translator_turnaround.rename(columns={\n",
                "    \"median\": \"MEDIAN_TURNAROUND\",\n",
                "    \"mean\": \"MEAN_TURNAROUND\",\n",
                "    \"std\": \"STD_TURNAROUND\",\n",
                "    \"count\": \"TASK_COUNT\"\n",
                "})\n",
                "\n",
                "# Filter to translators with at least 5 tasks\n",
                "df_translator_turnaround = df_translator_turnaround[df_translator_turnaround[\"TASK_COUNT\"] >= 5]\n",
                "\n",
                "print(\"Top translators by median turnaround time (fastest first):\")\n",
                "display(df_translator_turnaround.sort_values(\"MEDIAN_TURNAROUND\").head(20))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = df_data.dropna(subset=[\"COST\"])\n",
                "task_types = data[\"TASK_TYPE\"].unique()\n",
                "box_data = [data[data[\"TASK_TYPE\"] == task][\"COST\"] for task in task_types]\n",
                "\n",
                "# Create the boxplot\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.boxplot(box_data, \n",
                "            tick_labels=task_types,\n",
                "            patch_artist=True, # Enables filling the boxes\n",
                "            boxprops=dict(alpha=0.75),  # Set color and transparency\n",
                "            medianprops=dict(color=\"black\"))\n",
                "\n",
                "plt.title(\"Cost Distribution by Task Type\")\n",
                "plt.xlabel(\"Task Type\")\n",
                "plt.ylabel(\"Cost\")\n",
                "plt.ylim(0, 200)\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze quality by task type\n",
                "data=df_data.dropna(subset=[\"QUALITY_EVALUATION\"])\n",
                "task_types = data[\"TASK_TYPE\"].unique()\n",
                "box_data = [data[data[\"TASK_TYPE\"] == task][\"QUALITY_EVALUATION\"] for task in task_types]\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.boxplot(box_data, \n",
                "            tick_labels=task_types,\n",
                "            patch_artist=True, # Enables filling the boxes\n",
                "            boxprops=dict(alpha=0.75),  # Set color and transparency\n",
                "            medianprops=dict(color=\"black\"))\n",
                "plt.title(\"Quality Evaluation by Task Type\")\n",
                "plt.xlabel(\"Task Type\")\n",
                "plt.ylabel(\"Quality Evaluation\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive Language Pair Analysis\n",
                "df_data[\"LANGUAGE_PAIR\"] = df_data[\"SOURCE_LANG\"] + \" > \" + df_data[\"TARGET_LANG\"]\n",
                "language_pair_counts = df_data[\"LANGUAGE_PAIR\"].value_counts()\n",
                "\n",
                "top_language_pairs = language_pair_counts[:20] # Limited\n",
                "top_language_pairs_percentage = top_language_pairs / len(df_data) * 100\n",
                "\n",
                "print(\"Top Language Pairs:\")\n",
                "for i, (pair, count) in enumerate(top_language_pairs.items()):\n",
                "    percentage = top_language_pairs_percentage[pair]\n",
                "    print(f\"{i+1}. {pair}: {count} tasks ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize Top Language Pairs\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.barplot(x=top_language_pairs.values, y=top_language_pairs.index, alpha=0.75)\n",
                "\n",
                "\n",
                "# Add only percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / df_data.shape[0]\n",
                "    ax.text(width + 125, p.get_y() + p.get_height()/2, f\"{percentage:.1f}%\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.title(\"Top Language Pairs\")\n",
                "plt.xlabel(\"Count\")\n",
                "plt.ylabel(\"Language Pair\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.75)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze quality by language pair\n",
                "top_pairs_lang = top_language_pairs.index\n",
                "quality_by_pair = df_data[df_data[\"LANGUAGE_PAIR\"].isin(top_pairs_lang)].dropna(subset=[\"QUALITY_EVALUATION\"])\n",
                "\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.boxplot(x=\"LANGUAGE_PAIR\", y=\"QUALITY_EVALUATION\", \n",
                "                data=quality_by_pair, \n",
                "                order=top_pairs_lang,\n",
                "                boxprops=dict(alpha=0.75)\n",
                ")\n",
                "plt.title(\"Quality Evaluation by Top Language Pairs\")\n",
                "plt.xlabel(\"Language Pair\")\n",
                "plt.ylabel(\"Quality Evaluation\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze cost by language pair\n",
                "cost_by_pair = df_data[df_data[\"LANGUAGE_PAIR\"].isin(top_pairs_lang)].dropna(subset=[\"COST\"])\n",
                "\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.boxplot(x=\"LANGUAGE_PAIR\", y=\"COST\",\n",
                "                data=cost_by_pair,\n",
                "                order=top_pairs_lang,\n",
                "                boxprops=dict(alpha=0.75)\n",
                ")\n",
                "plt.title(\"Cost Distribution by Top Language Pairs\")\n",
                "plt.xlabel(\"Language Pair\")\n",
                "plt.ylabel(\"Cost\")\n",
                "plt.ylim(0, 200)\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze hourly rate by language pair\n",
                "hourly_by_pair = df_data[df_data[\"LANGUAGE_PAIR\"].isin(top_pairs_lang)].dropna(subset=[\"HOURLY_RATE\"])\n",
                "\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.boxplot(x=\"LANGUAGE_PAIR\", y=\"HOURLY_RATE\", \n",
                "                data=hourly_by_pair,\n",
                "                order=top_pairs_lang,\n",
                "                boxprops=dict(alpha=0.75)\n",
                ")\n",
                "plt.title(\"Hourly Rate Distribution by Top Language Pairs\")\n",
                "plt.xlabel(\"Language Pair\")\n",
                "plt.ylabel(\"Hourly Rate\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze industry sectors\n",
                "sector_counts = df_data[\"MANUFACTURER_SECTOR\"].value_counts()\n",
                "top_sectors = sector_counts.head(15)\n",
                "top_sectors_percentage = top_sectors / len(df_data) * 100\n",
                "\n",
                "print(\"Top Manufacturer Sectors:\")\n",
                "for i, (sector, count) in enumerate(top_sectors.items()):\n",
                "    percentage = top_sectors_percentage[sector]\n",
                "    print(f\"{i+1}. {sector}: {count} tasks ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize Top Sectors\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.barplot(x=top_sectors.values, y=top_sectors.index, alpha=0.75)\n",
                "\n",
                "# Add count and percentage labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    width = p.get_width()\n",
                "    percentage = 100 * width / df_data.shape[0]\n",
                "    ax.text(width + 125, p.get_y() + p.get_height()/2, f\"{percentage:.1f}%\", ha=\"left\", va=\"center\")\n",
                "\n",
                "plt.title(\"Top Manufacturer Sectors\")\n",
                "plt.xlabel(\"Count\")\n",
                "plt.ylabel(\"Manufacturer Sector\")\n",
                "plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.75)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze task duration and workflow in different stages\n",
                "df_data[\"ASSIGNMENT_TO_READY\"] = (df_data[\"READY\"] - df_data[\"ASSIGNED\"]).dt.total_seconds() / 3600\n",
                "df_data[\"READY_TO_WORKING\"] = (df_data[\"WORKING\"] - df_data[\"READY\"]).dt.total_seconds() / 3600\n",
                "df_data[\"WORKING_TO_DELIVERED\"] = (df_data[\"DELIVERED\"] - df_data[\"WORKING\"]).dt.total_seconds() / 3600\n",
                "df_data[\"DELIVERED_TO_RECEIVED\"] = (df_data[\"RECEIVED\"] - df_data[\"DELIVERED\"]).dt.total_seconds() / 3600\n",
                "df_data[\"RECEIVED_TO_CLOSE\"] = (df_data[\"CLOSE\"] - df_data[\"RECEIVED\"]).dt.total_seconds() / 3600\n",
                "df_data[\"TOTAL_DURATION\"] = (df_data[\"CLOSE\"] - df_data[\"ASSIGNED\"]).dt.total_seconds() / 3600\n",
                "\n",
                "# Filter out negative durations or extreme outliers\n",
                "duration_cols = [\n",
                "    \"ASSIGNMENT_TO_READY\", \n",
                "    \"READY_TO_WORKING\", \n",
                "    \"WORKING_TO_DELIVERED\", \n",
                "    \"DELIVERED_TO_RECEIVED\",\n",
                "    \"RECEIVED_TO_CLOSE\",\n",
                "    \"TOTAL_DURATION\"\n",
                "]\n",
                "\n",
                "for col in duration_cols:\n",
                "    df_data = df_data[(df_data[col] >= 0) | (df_data[col].isnull())]\n",
                "    upper_limit = df_data[col].quantile(0.99)\n",
                "    df_data.loc[df_data[col] > upper_limit, col] = np.nan\n",
                "    # Obtain number of filtered\n",
                "\n",
                "# Summary statistics for workflow durations\n",
                "print(\"Workflow Duration Statistics (hours):\")\n",
                "duration_stats = df_data[duration_cols].describe().T\n",
                "duration_stats[\"% of Total\"] = (duration_stats[\"mean\"] / df_data[\"TOTAL_DURATION\"].mean()) * 100\n",
                "display(duration_stats)\n",
                "\n",
                "# Visualize workflow stage durations\n",
                "plt.figure(figsize=(14, 8))\n",
                "workflow_means = df_data[duration_cols[:-1]].mean()\n",
                "workflow_means = workflow_means.reindex(duration_cols[:-1])  # Ensure correct order\n",
                "\n",
                "ax = sns.barplot(x=workflow_means.index, y=workflow_means.values, edgecolor=\"black\", alpha=0.75)\n",
                "plt.title(\"Average Duration by Workflow Stage\")\n",
                "plt.xlabel(\"Workflow Stage\")\n",
                "plt.ylabel(\"Average Duration (hours)\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.75)\n",
                "plt.xticks(rotation=45)\n",
                "# Add duration labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    height = p.get_height()\n",
                "    percentage = 100 * height / duration_stats.loc[\"TOTAL_DURATION\", \"mean\"]\n",
                "    ax.text(p.get_x() + p.get_width()/2, height + 0.1, f\"{percentage:.1f}%\", ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze workflow duration by task type\n",
                "workflow_by_task = df_data.groupby(\"TASK_TYPE\")[duration_cols].mean()\n",
                "print(\"Workflow Duration by Task Type (hours):\")\n",
                "display(workflow_by_task)\n",
                "\n",
                "# Visualize working time by task type\n",
                "plt.figure(figsize=(14, 8))\n",
                "ax = sns.barplot(x=workflow_by_task.index, y=workflow_by_task[\"WORKING_TO_DELIVERED\"], edgecolor=\"black\", alpha=0.75)\n",
                "plt.title(\"Average Working Time by Task Type\")\n",
                "plt.xlabel(\"Task Type\")\n",
                "plt.ylabel(\"Average Working Time (hours)\")\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.75)\n",
                "\n",
                "# Add duration labels\n",
                "for i, p in enumerate(ax.patches):\n",
                "    height = p.get_height()\n",
                "    ax.text(p.get_x() + p.get_width()/2, height + 0.1, f\"{height:.1f}h\", ha=\"center\", va=\"bottom\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze translator workload and performance\n",
                "translator_task_counts = df_data[\"TRANSLATOR\"].value_counts()\n",
                "translator_stats = df_data.groupby(\"TRANSLATOR\").agg({\n",
                "    \"TASK_ID\": \"count\",\n",
                "    \"QUALITY_EVALUATION\": [\"mean\", \"std\", \"count\"],\n",
                "    \"COST\": [\"mean\", \"sum\"],\n",
                "    \"WORKING_TO_DELIVERED\": [\"mean\", \"std\"],\n",
                "    \"LANGUAGE_PAIR\": lambda x: x.nunique()\n",
                "}).round(2)\n",
                "\n",
                "translator_stats.columns = [\n",
                "    \"Task_Count\", \n",
                "    \"Avg_Quality\", \"Quality_Std\", \"Quality_Count\", \n",
                "    \"Avg_Cost\", \"Total_Cost\",\n",
                "    \"Avg_Working_Time\", \"Working_Time_Std\",\n",
                "    \"Language_Pair_Count\"\n",
                "]\n",
                "\n",
                "translator_stats = translator_stats.sort_values(\"Task_Count\", ascending=False)\n",
                "\n",
                "print(\"Top Translator Statistics:\")\n",
                "display(translator_stats.head(20))\n",
                "\n",
                "# Visualize translator task counts\n",
                "plt.figure(figsize=(16, 10))\n",
                "ax = sns.barplot(x=translator_task_counts.head(20).values, y=translator_task_counts.head(20).index, edgecolor=\"black\", alpha=0.75)\n",
                "plt.title(\"Top Translators by Number of Tasks\")\n",
                "plt.xlabel(\"Translator\")\n",
                "plt.ylabel(\"Number of Tasks\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze relationship between task count and quality\n",
                "plt.figure(figsize=(14, 8))\n",
                "plt.scatter(\n",
                "    translator_stats[\"Task_Count\"], \n",
                "    translator_stats[\"Avg_Quality\"], \n",
                "    alpha=0.35,\n",
                "    s=translator_stats[\"Quality_Count\"]  # Size based on number of quality evaluations\n",
                ")\n",
                "plt.title(\"Relationship Between Task Count and Average Quality\")\n",
                "plt.xlabel(\"Number of Tasks\")\n",
                "plt.ylabel(\"Average Quality Evaluation\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze top translators by language pair\n",
                "top_translators = translator_stats.head(10).index\n",
                "language_pairs_by_translator = df_data[df_data[\"TRANSLATOR\"].isin(top_translators)].groupby(\n",
                "    [\"TRANSLATOR\", \"LANGUAGE_PAIR\"]\n",
                ").size().reset_index(name=\"count\")\n",
                "\n",
                "# For each top translator, show their three top language pairs\n",
                "top_translator_pairs = []\n",
                "for translator in top_translators:\n",
                "    translator_pairs = language_pairs_by_translator[language_pairs_by_translator[\"TRANSLATOR\"] == translator]\n",
                "    translator_pairs = translator_pairs.sort_values(\"count\", ascending=False).head(3)\n",
                "    for _, row in translator_pairs.iterrows():\n",
                "        top_translator_pairs.append({\n",
                "            \"TRANSLATOR\": row[\"TRANSLATOR\"],\n",
                "            \"LANGUAGE_PAIR\": row[\"LANGUAGE_PAIR\"],\n",
                "            \"COUNT\": row[\"count\"]\n",
                "        })\n",
                "\n",
                "translator_top_pairs = pd.DataFrame(top_translator_pairs)\n",
                "print(\"Top Language Pairs for Top Translators:\")\n",
                "display(translator_top_pairs)\n",
                "\n",
                "# Language pair distribution for each language pair\n",
                "unique_language_pairs = translator_top_pairs[\"LANGUAGE_PAIR\"].unique()\n",
                "\n",
                "for language_pair in unique_language_pairs:\n",
                "    data = translator_top_pairs[translator_top_pairs[\"LANGUAGE_PAIR\"] == language_pair]\n",
                "    \n",
                "    # Sort data by COUNT to ensure color mapping makes sense\n",
                "    data = data.sort_values(\"COUNT\")\n",
                "    \n",
                "    # Color palette based on the COUNT values\n",
                "    norm = plt.Normalize(data[\"COUNT\"].min(), data[\"COUNT\"].max())\n",
                "    colors = plt.cm.YlGnBu(norm(data[\"COUNT\"]))\n",
                "    \n",
                "    plt.figure(figsize=(14, 8))\n",
                "    plt.barh(data[\"TRANSLATOR\"], data[\"COUNT\"], color=colors, edgecolor=\"black\", alpha=0.75)\n",
                "    plt.title(f\"Top Translators for Language Pair: {language_pair}\")\n",
                "    plt.xlabel(\"Number of Tasks\")\n",
                "    plt.ylabel(\"Translator\")\n",
                "    plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.75)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For each sector, identify the most commonly assigned translators\n",
                "sector_translator = df_data.groupby([\"MANUFACTURER_SECTOR\", \"TRANSLATOR\"]).size().reset_index(name=\"count\")\n",
                "top_sector_translators = []\n",
                "\n",
                "for sector in top_sectors.index[:5]:  # For top sectors\n",
                "    sector_data = sector_translator[sector_translator[\"MANUFACTURER_SECTOR\"] == sector]\n",
                "    sector_data = sector_data.sort_values(\"count\", ascending=False).head(5)\n",
                "    for _, row in sector_data.iterrows():\n",
                "        top_sector_translators.append({\n",
                "            \"SECTOR\": row[\"MANUFACTURER_SECTOR\"],\n",
                "            \"TRANSLATOR\": row[\"TRANSLATOR\"],\n",
                "            \"COUNT\": row[\"count\"]\n",
                "        })\n",
                "\n",
                "sector_top_translators = pd.DataFrame(top_sector_translators)\n",
                "print(\"Top Translators for Top Sectors:\")\n",
                "display(sector_top_translators)\n",
                "\n",
                "\n",
                "# Separate plots for each sector\n",
                "for sector in sector_top_translators[\"SECTOR\"].unique():\n",
                "    sector_data = sector_top_translators[sector_top_translators[\"SECTOR\"] == sector].sort_values(\"COUNT\", ascending=True)\n",
                "    \n",
                "    # Color palette based on the COUNT values\n",
                "    norm = plt.Normalize(sector_data[\"COUNT\"].min(), sector_data[\"COUNT\"].max())\n",
                "    colors = plt.cm.YlGnBu(norm(sector_data[\"COUNT\"]))\n",
                "    \n",
                "    plt.figure(figsize=(16, 8))\n",
                "    plt.barh(sector_data[\"TRANSLATOR\"], sector_data[\"COUNT\"], color=colors, edgecolor=\"black\", alpha=0.75)\n",
                "    plt.title(f\"Top Translators in {sector} Sector\")\n",
                "    plt.xlabel(\"Number of Tasks\")\n",
                "    plt.ylabel(\"Translator\")\n",
                "    plt.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.75)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze relationship between cost and quality\n",
                "quality_cost_df = df_data.dropna(subset=[\"COST\", \"QUALITY_EVALUATION\"])\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.scatterplot(x=\"COST\", y=\"QUALITY_EVALUATION\", data=quality_cost_df, alpha=0.5)\n",
                "plt.title(\"Relationship Between Cost and Quality Evaluation\")\n",
                "plt.xlabel(\"Cost\")\n",
                "plt.ylabel(\"Quality Evaluation\")\n",
                "plt.xlim(0, quality_cost_df[\"COST\"].quantile(0.99))  # Exclude extreme outliers\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate correlation between cost and quality\n",
                "cost_quality_corr = quality_cost_df[[\"COST\", \"QUALITY_EVALUATION\"]].corr().iloc[0, 1]\n",
                "print(f\"Correlation between Cost and Quality Evaluation: {cost_quality_corr:.4f}\")\n",
                "\n",
                "# Analyze relationship between hourly rate and quality\n",
                "hourly_quality_df = df_data.dropna(subset=[\"HOURLY_RATE\", \"QUALITY_EVALUATION\"])\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.scatterplot(x=\"HOURLY_RATE\", y=\"QUALITY_EVALUATION\", data=hourly_quality_df, alpha=0.5)\n",
                "plt.title(\"Relationship Between Hourly Rate and Quality Evaluation\")\n",
                "plt.xlabel(\"Hourly Rate\")\n",
                "plt.ylabel(\"Quality Evaluation\")\n",
                "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate correlation between hourly rate and quality\n",
                "hourly_quality_corr = hourly_quality_df[[\"HOURLY_RATE\", \"QUALITY_EVALUATION\"]].corr().iloc[0, 1]\n",
                "print(f\"Correlation between Hourly Rate and Quality Evaluation: {hourly_quality_corr:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_data[\"PLANNED_DURATION\"] = (df_data[\"END\"] - df_data[\"START\"]).dt.total_seconds() / 3600\n",
                "df_data[\"PLANNED_VS_ACTUAL\"] = df_data[\"PLANNED_DURATION\"] - df_data[\"WORKING_TO_DELIVERED\"]\n",
                "df_data[\"ON_TIME\"] = np.where(df_data[\"PLANNED_VS_ACTUAL\"] >= 0, \"Yes\", \"No\")\n",
                "\n",
                "on_time_counts = df_data[\"ON_TIME\"].value_counts()\n",
                "on_time_percentage = on_time_counts / on_time_counts.sum() * 100\n",
                "\n",
                "print(\"\\nTask Completion Analysis:\")\n",
                "for status, count in on_time_counts.items():\n",
                "    percentage = on_time_percentage[status]\n",
                "    print(f\"- {status}: {count} tasks ({percentage:.2f}%)\")\n",
                "\n",
                "# Visualize on-time performance by task type\n",
                "on_time_by_task = pd.crosstab(df_data[\"TASK_TYPE\"], df_data[\"ON_TIME\"], normalize=\"index\") * 100\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "on_time_by_task[\"Yes\"].sort_values(ascending=False).plot(kind=\"bar\", alpha=0.75)\n",
                "plt.title(\"On-Time Completion Percentage by Task Type\")\n",
                "plt.xlabel(\"Task Type\")\n",
                "plt.ylabel(\"On-Time Percentage (%)\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Visualize on-time performance by language pair (top)\n",
                "on_time_by_pair = df_data[df_data[\"LANGUAGE_PAIR\"].isin(top_pairs_lang)]\n",
                "on_time_by_pair = pd.crosstab(on_time_by_pair[\"LANGUAGE_PAIR\"], on_time_by_pair[\"ON_TIME\"], normalize=\"index\") * 100\n",
                "\n",
                "plt.figure(figsize=(14, 8))\n",
                "on_time_by_pair[\"Yes\"].sort_values(ascending=False).plot(kind=\"barh\", alpha=0.75)\n",
                "plt.title(\"On-Time Completion Percentage by Top Language Pairs\")\n",
                "plt.xlabel(\"Language Pair\")\n",
                "plt.ylabel(\"On-Time Percentage (%)\")\n",
                "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"KEY FINDINGS FROM SAMPLE ANALYSIS:\")\n",
                "print()\n",
                "print(\"Tasks and Projects:\")\n",
                "print(f\"    {df_data.shape[0]} tasks across {df_data['PROJECT_ID'].nunique()} projects\")\n",
                "print()\n",
                "print(\"Translator Information:\")\n",
                "print(f\"    {df_data['TRANSLATOR'].nunique()} unique translators\")\n",
                "print(f\"    {translator_task_counts.mean():.2f} tasks per translator on average\")\n",
                "print()\n",
                "print(\"Task Types:\")\n",
                "print(f\"    Most common: '{task_type_counts.index[0]}' ({task_type_percentage[task_type_counts.index[0]]:.2f}% of all tasks)\")\n",
                "print()\n",
                "print(\"Language Pairs:\")\n",
                "print(f\"    Most common: {language_pair_counts.index[0]} ({top_language_pairs_percentage[language_pair_counts.index[0]]:.2f}% of all tasks)\")\n",
                "print()\n",
                "print(\"Quality Metrics:\")\n",
                "print(f\"    Average quality evaluation: {df_data['QUALITY_EVALUATION'].mean():.2f}/10\")\n",
                "print()\n",
                "print(\"Financial Metrics:\")\n",
                "print(f\"    Average task cost: {df_data['COST'].mean():.2f}€\")\n",
                "print(f\"    Average hourly rate: {df_data['HOURLY_RATE'].mean():.2f}€\")\n",
                "print()\n",
                "print(\"Workflow Analysis:\")\n",
                "print(f\"    Most time-consuming stage: '{duration_stats.iloc[duration_stats['mean'].argmax()].name}' ({duration_stats['mean'].max():.2f} hours)\")\n",
                "print()\n",
                "print(\"Performance Metrics:\")\n",
                "print(f\"    On-time completion rate: {on_time_percentage['Yes']:.2f}%\")\n",
                "print()\n",
                "print(\"Correlation Analysis:\")\n",
                "print(f\"    Cost and quality correlation: {cost_quality_corr:.4f}\")\n",
                "print(f\"    Hourly rate and quality correlation: {hourly_quality_corr:.4f}\")\n",
                "print()\n",
                "print(\"Industry Information:\")\n",
                "print(f\"    Top industry sector: '{sector_counts.index[0]}' ({top_sectors_percentage[sector_counts.index[0]]:.2f}% of all tasks)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Synthesis",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
