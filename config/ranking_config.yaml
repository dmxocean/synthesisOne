project:
  name: "Translation"
  team: "synthesisOne"
  version: "1.0"

model:
  name: "lgbm_ranker"
  version: "1.0"

parameters:
  objective: "lambdarank"
  metric: "ndcg"
  eval_at: [1, 3, 5, 10]
  boosting_type: "gbdt"
  learning_rate: 0.05
  num_leaves: 125
  max_depth: 120
  min_data_in_leaf: 5
  min_sum_hessian_in_leaf: 0.001
  feature_fraction: 0.75
  bagging_fraction: 0.45
  bagging_freq: 5
  lambda_l1: 0.25
  lambda_l2: 0.25
  random_seed: 42
  force_row_wise: true

training:
  num_boost_round: 500
  early_stopping_rounds: 15
  verbose_eval: 50

prediction:
  top_k: 10

evaluation:
  metrics: ["ndcg", "precision", "mrr"]
  k_values: [1, 3, 5, 10]